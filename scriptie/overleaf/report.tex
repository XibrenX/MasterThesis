\documentclass{ou-report}
%% \citestyle{agu}

\newcommand{\HJ}[1]{{\color{red} HJ: #1}}
\newcommand{\todo}[1]{{\color{red} TODO: #1}}
\newcommand{\vraag}[1]{{\color{teal} {\textbf{VRAGEN AAN HUGO: }}#1}}
\newcommand{\outline}[1]{{\color{blue} #1}}
\newcommand{\old}[1]{{\color{gray} #1}}

\usepackage{float}
\usepackage{tikz}
\usepackage[shortlabels]{enumitem}
\usepackage[normalem]{ulem}

\usetikzlibrary{positioning}
\usepackage{graphicx}

% Settings for code fragments
\lstset{
  basicstyle=\fontsize{9}{13}\selectfont\ttfamily
}

% Dit template is gemaakt door P.J. Molijn in het kader van zijn afstuderen aan de OU in 2014.
% Waarvoor hartelijk dank.
% Minieme maar belangrijke wijzigingen zijn aangebracht door E.M. van Doorn
% Het template is versimpeld door Sylvia Stuurman, 2019.


%\hypersetup{
%pdfsubject={Master Thesis <Titel>, <author>},
%pdfkeywords={keyword1, keyword2}
%}

\begin{document}
\pagestyle{plain}
%% \title{Applying outlier detection on the scientific publication process to support fraud detection}
\title{Acquisition and integration of public data to improve detection of scientific fraud}
%% \title{Improving detection of scientific fraud by enriching the publication model with public data}
\author{Ewoud Westerbaan}
%Title of the thesis
%\title[Subtitle]{Title}
%\author{author}
%\affiliation{
%\begin{tabular}{ll}
%Student: & studentnumber \\
%Date:    & DD/MM/YYY \\
%\end{tabular}
%}
%
%%\coverimage{cover/cover.jpg}
%%            ===============
%\makecover[frontboxwidth=4.6in]
\input{title}

%% Use Roman numerals for the page numbers of the title pages and table of
%% contents.
\pagenumbering{roman}
%% Include an optional title page.

\frontmatter 


\let\cleardoublepage\clearpage

% Optional Dedication, Acknowledgement
%\input{dedication}

%\input{acknowledgement}
\tableofcontents

%Optional: list of figures, list of tables
%\listoffigures

%\listoftables

%% Include an optional summary page.
%\input{Summary/summary}
%\input{Summary/samenvatting}

\mainmatter
\pagenumbering{arabic}


\newcommand{\mi}[1]{\ensuremath{\mathit{#1}}}
\newcommand{\authors}{\mi{authors}}
\newcommand{\cites}{\mi{cites}}
\newcommand{\receives}{\mi{receives}}
\newcommand{\reviews}{\mi{reviews}}
\newcommand{\accepts}{\mi{accepts}}
\newcommand{\rejects}{\mi{rejects}}
\newcommand{\editorinchief}{\mi{EiC}}
\newcommand{\associateeditor}{\mi{AE}}
\newcommand{\Humans}{\mi{Humans}}
\newcommand{\Reviewers}{\mi{Reviewers}}
\newcommand{\Editors}{\mi{Editors}}


% ==============================================================================
\chapter*{Abstract}
% ==============================================================================
% Placeholder

\outline{
\begin{itemize}
    \item Geen nummering
    \item voor introductie
    \item Probleem
    \item Main contributions
    \item Main results
\end{itemize}
}

% ==============================================================================
\chapter{Introduction}
% ==============================================================================
\outline{
\begin{itemize}
    \item Wat moet een informaticus met een andere focus weten om deze scriptie 
        te kunnen lezen.
    \item er bestaat fraude, sommige worden gevonden, blabla (smeuig maken)
\end{itemize}
}
Incentives for scientists rely on quantitative measures of science, such as 
number of publications, number of citations and amount of grants received. 
However, as the aphorism known as Goodhart's Law states: ``When a measure 
becomes a target, it ceases to be a good measure''~\cite{strathern_1997}. That 
is the case here as well: these metrics not only incentivise scientific 
excellence by doing research and publishing the results; they also invite, 
maybe even more rewarding, dishonest approaches to achieve high rankings 
-- gaming of these metrics, or, more simply, fraud in scientific publishing. 

A perfect example of Goodhart's Law is the following case. Since 2010 in 
Italy the number of citations is used as input for promotion. Seeber et al.
concluded that this threshold increased the number of 
self-citation~\cite{SEEBER2019478}.

Other examples of fraud include plagiarism, manipulation of images (e.g., of cell 
extracts in life science publications), manipulation of research data, 
fabrication of research data. In addition, there may be as-of-yet unknown forms 
of fraud.

Various prevention and detection measures are in place. In absence of these 
deterrents, the scientific process would be vulnerable to dishonest behaviour. 
These existing deterrents are typically focused on specific forms of dishonest 
behaviour. Examples of these existing measures are Ithenticate to detect 
plagiarism, or ORI's Forensic Droplets\footnote{\url{https://ori.hhs.gov/droplets}} 
to detect manipulated images. The practice of preregistration ensures scientific 
studies are conducted as planned, and help to prevent data manipulation such as 
p-hacking\footnote{P-hacking is manipulation of the data so the research becomes 
statistically important.}. Another measure is applying statistical evaluation to 
uncover manipulation of research data~\cite{HGWA2019}. 

Despite these deterrents, some forms of dishonest behaviour still slip through.
% \HJ{eventueel hier enkele aanvallen noemen.} 
Examples of such behaviour are;
self-nominating as reviewer (Moon), forced citation (authors are being forced
to cite work), forced co-author-ship (forced to name a person as co-author).
Such behaviours use dishonest tricks outside the detective capabilities of
current detection methods and therefore escape initial scrutiny. We know of
such cases typically through whistle blowers and manual investigation. 
This illustrates that a problem still exists, yet the scale of the
problem remains unknown.

In particular, one obstacle to prevent this problem is the huge (and growing) 
number of scientific 
publications and scientific authors. For example, the DBLP website, which 
records most publications in computer science, currently lists more than 
2.5~million authors -- a number that cannot be manually investigated. Hence, a 
more generic approach to detect fraud is needed (not focused on a certain type
of attack).


% \todo{
% \begin{itemize}
%     \item dus: Generieke vorm van detectie gewenst
%     \item dat kan dus niet afhangen van een specifieke aanval
%     \item dat moet dus gebaseerd zijn op een holistische view van het 
%         publicatieproces (kerngetallen bijvoorbeeld)
%         -- en weten wat daar de norm is.
%     \item Dus beschouwen we in de volgende secties het publicatieprocess in meer 
%         detail.
% \end{itemize}
% }
% ==============================================================================
In this project, we aim to take a step in this generic approach by not 
focusing on the method of fraud applied, but on the 
\emph{effect of scientific fraud}, by taking a holistic view of the publication 
process. We tend to do this in two steps:
\begin{enumerate}
\item Provide a concise underlaying integrated datamodel to support fraud 
detection.
\item Identify outliers within groups sharing characteristics.
\end{enumerate}
% We tend to do this in two 
% steps:
% \begin{enumerate}
%     \item The first step is to identify groups of people sharing a 
%         charateristic (e.g. same role).
%     \item The second step is to identify outliers within this group based on
%         other characteristics (e.g. number of publications).
% \end{enumerate}
% Other examples are given in 
% Section~\ref{interesting_cases}.

The underlying assumption this approach is based upon, is that the various types 
of scientific fraud all 
share a common characteristic: their goal is the same, that is, their goal 
is to improve a researcher's or publication's quantitative measures of 
science. This elevates them above their peers -- which on the one hand 
brings recognition and accolades, but on the other, makes them stand out. In 
short, the underlying assumption is that fraudsters aim to become outliers 
(in terms of quantitative measures of science).

To clarify this with an example: An editor of a journal publishes in his 
own journal. This is not a strange fact on itself. However, if other editors of
this same journal do not publish (or publish much less), this editor of interest
can considered being an outlier, because of the discrepancy with his group 
of peers. 

We immediately caution that this implication ($fraud \implies outlier$) does 
not hold in reverse. That is: such outliers need not be fraudsters. 

However, identifying outliers may reduce the pool of authors to be investigated
by se\-veral orders of magnitude. Thus, while not perfect, outlier classification
should make manual investigation of cases where fraud would have had significant
effect possible.

The goal of this project is to take a first step to make this a possibility.
Concretely, we will integrate multiple sources which provides the possibility
to perform a group based outlier detection.


% perform a group based approach on the publication process
% to analyse if an group based outlier detection is a feasable option to focus
% the limit the capacity of manual investigation to strongly 
% increase the probability of identifying the most egregious of fraudsters. 
% Concretely, we will design a generic detection method which uses the metrics
% of the publication process and the incentives of the authors. 
% Our proposal is to 
% use machine learning techniques to detect authors which are outliers, given the 
% metrics of the publication process. The goal we aim to achieve with our proposal 
% is to focus the limited capacity of manual fraud investigation to strongly 
% increase the probability of identifying the most egregious of fraudsters. 

% ------------------------------------------------------------------------------
The focus of this study is on the computer science 
discipline. For other disciplines this method probably to be tuned specifically. 
For example: the average number of co-authors per article change between research 
areas\footnote{\url{https://www.natureindex.com/news-blog/paper-authorship-goes-hyper}}.

% \outline{
% \paragraph{Tweetrapsraketten}
% % -------------------------------------
% Idee: (1) vind groep personen; (2) vind outliers binnen die groep
% \begin{itemize}
% \item Editors van hetzelfde journal $\implies$ \#pubs in journal
% \item Mensen met zelfde H-index  $\implies$
%     \begin{itemize}
%     \item \#publicaties
%     \item \#jaren actief
%     \item \#citaties in h-core (hoeveel papers hebben minimale score) \\
%         Dit is een beetje: hoe verloopt de citatie``curve'' vergeleken met anderen
%     \item ranking van venues in h-core
%         \begin{itemize}
%         \item In vergelijking met anderen
%         \item In vergelijking met papers buiten je eigen h-core \\
%             en dan ook in vergelijking met anderen
%         \end{itemize}
        
%     \end{itemize}
% \end{itemize}
% }
\paragraph{}
% \outline{
% \begin{itemize}
%     \item Wat gaan wij bijdragen en waarom is dit cool? -> Motivatie
%     \item +/- 1 contributie per hoofdstuk
% \end{itemize}
% }
During this research we will contribute the following items:
\begin{description}
    \item[Formal datamodel] which can be used to reason about the publication model.
        Sources to load this model are discussed.
    \item[Acquisition methods] to enrich existing publicly available datasets with
        non machine-readable data.
    \item[Case studies] using a group based approach to identify outliers.
    \item[Recommendations] to increase the detection of fraud. 
    % \item[Situations of dishonest behaviour] which we want to identify using our 
    %     methodology; the attacks.
    % \item[A formal model of the publication process] which contains the objects 
    %     necessary to catch the identified attacks.
    % \item[Data collection methodology] to get the necessary data to provide an 
    %     holistic view on the publication process.
\end{description}
% \old{
% \begin{description}
%     \item[A formal model of the publication process] The publication process 
%         consists of inputs (features) and outputs (metrics). Simply defined; the 
%         output is depended on the inputs, and those inputs are a result of the 
%         behaviour of the author. The output is the metric the author want to 
%         change in his (or her) favor. To determine what features we need to 
%         monitor, we need to understand how this publication process works. To 
%         achieve this, we need to have a formal model of the process to analyse 
%         these relations, which is our second contribution:
%     \item[Deriving input-output relations] Given this formal model, we can 
%         derive all inputs that affect the output metrics, which are the metrics 
%         an attacker wants to adjust. This is important because these relations 
%         give us insights how the publication model can be played. With the 
%         inputs of these relations we can start putting together a list of 
%         features we need to collect. We will formalise a few metrics and 
%         elaborate how to game this model to adjust them in favor of the 
%         attacker. Except these theoretical gaming, we also know some attacks on 
%         the model which actually did took place, which leads to our third 
%         contribution.
%     \item[Overview of known attacks] We investigate known attacks and show how 
%         to model these attacks in the formal model. This substantiates that the 
%         formal model is relevant for detecting practical attacks. Combining 
%         these with the formal attacks brings us to our fourth contribution.
%     \item[Features indicating an attack] We combine the knowledge gained from 
%         establishing input-output relations and the theoretical embedding of 
%         practical attacks in the formal model to derive features that are 
%         indicative of metrics, and therefore, possibly abused in attacks. Having 
%         this features, we can proceed with the detection method.
%     \item[Detection method] With these features in hand we propose a formal 
%         approach to automatically separate ``interesting'' cases from regular 
%         scientists. Consider these features as input for our detection 
%         mechanism, the output is a set of regular authors and a set of authors 
%         that are positive outliers on (combinations of) those features.
% \end{description}
% }


    
% This research proposal continues in Section~\ref{RelatedWork}, 
% \nameref{RelatedWork}, where we discuss publications which are related to the 
% goal we want to achieve. A major topic in this project is outlier (or anomaly)
% detection; we will discuss this topic and how this incorporates in this project 
% in Section~\ref{anomaly_detection}. Next, we describe our research questions in
% Section~\ref{Research_ResearchQuestions} and how we think we should be able to 
% answer them in Section~\ref{Research_ResearchMethod}. Because the research we 
% propose is very dependent on data, we provide an initial investigation of 
% possible data sources. The outcome and result of this analysis is described in 
% Section~\ref{Research_Data} '\nameref{Research_Data}'. The validity of our 
% research is discussed in Section~\ref{Research_Validation}. The research
% proposal concludes with a planning in Section~\ref{Planning} and risks and 
% mitigation strategies in Section~\ref{Planning_RiskAnalysis}.

\paragraph{}
This thesis will continue with a domain analysis of the publication process in
Chapter~\ref{chp:domainanalysis}. After this background chapter, other 
resources related are discussed in Chapter~\ref{chp:related_work}. The used
methodology is described in Chapter~\ref{chp:methodology}. 

From here on we will
answer the research questions which starts with the datamodel and - acquisition
in Chapter~\ref{chp:data} and follows with three case studies in 
Chapters~\ref{chp:case1}, \ref{chp:case2} and \ref{chp:case3}. 
Chapter~\ref{chp:front_matter_parsing} is an addition to the case study in 
Chapter~\ref{chp:case1}.

We end this thesis with the conclusions in Chapter~\ref{chp:conclusions} and
recommendations in Chapters~\ref{chp:recommendations}.



% ==============================================================================
\chapter{Domain analysis of the publication process}
\label{chp:domainanalysis}
% ==============================================================================
% \outline{
% \begin{itemize}
%     \item Beginnen met verhaal over publicatieprocess
%     \item Reden: wetenschapper wilt publiceren
% \end{itemize}
% }

Incentives for scientists rely on quantitative measures of science, such as 
number of publications, number of citations and amount of grants received. The
result of these measures being in benefit for the scientist are a higher status, 
salary and more resources (funds, students) to do 
more research which leads to even more rewards, recognition and accolades. To 
achieve these rewards, scientists need to have their papers published for their 
career to thrive. This process of publishing and rewarding is shown by Bj\"ork 
and Hedlund~\cite{BH2004} in a cycle within their visualisation of the 
publication process. In Figure~\ref{fig:publish_incentives} we present this 
cycle.

\begin{figure}[H]
\centering
\includegraphics[width=12cm]{images/publication_process.drawio.png}
\caption{Publish for incentives (adapted from~\cite{BH2004})}
\label{fig:publish_incentives}
\end{figure}
% ==============================================================================

In this image we see that the researcher is evaluated and receives incentives 
(such as funds) based on performance measures. In their paper,  Bj\"ork and 
Hedlund mention that for public bodies that provide funds (which is an incentive 
for the researcher), the production and consumption of publications should be 
optimized.
% For researchers, there are multiple strategies to achieve appointments and grants.
% Interestingly, only one of these is to act honestly, that is, to perform the
%     research and publish results. Dishonest approaches are possible,
%     and may even be more rewarding. In absence of deterrents, the scientific
%     process would be inundated by this.
%     \todo{naar intro}
% Existing deterrents are typically focused on specific forms of dishonest
%     behaviour. However, dishonest behaviour still occurs.
    


%% For an author this is an interesting perspective. What can we consider 
%%     production and what consumption and, for detecting fraud, how can we pretend 
%%     to produce and pretend the work is being consumed. In other words; how can 
%%     one pretend to be interesting and gain incentives.
%%     % \item It is save to consider bringing papers to the public as publication. In this chapter we will dive in how this process looks like and the roles and power involved; who has the power that your work can even be consumed?
Two main targets exists for the production part, (the scientific publication): 
journals (incollection) or 
conferences (inproceeding). While journal publications are the norm in many 
other disciplines, conference publications are common in Computer Science.

The next sections describe these processes from a Computer Science perspective. 
For other disciplines these processes may differ. For example, in CS 
conferences, rigorous peer-review is the norm, with acceptance rates of 20\% or 
less being common; in other disciplines, such strict peer review for conferences is
less common.
    
% In the next section we examine the process in more detail.


% ==============================================================================
\section{Journal publication}
% \outline{
% beschrijvend het process bespreken
% }

First we will discuss the process for publishing in a journal. Although the 
exact process may differ across journals, there is a certain common process. We 
tend to describe this common process.

The process of publishing in a journal starts with the author who has a manuscript 
he wants to have published. This can be based on a `call for papers', or on own 
initiative. According to Cormode, the Editor-in-Chief receives the manuscript 
and do a minor check if the paper is good enough for further processing. If it 
is, the further process of handling the paper is assigned to an associate editor 
who also perform some checks for quality (understandability, duplication of 
prior work, topic in scope). The checks from the editor-in-chief and associate 
editor can lead to a rejection of the manuscript. According to Cormode, the 
author is encouraged to suggest an associate editor at submitting. This helps 
the editor-in-chief to delegate the handling to a associate-editor with 
knowledge of the domain e.g..

The main responsibilities of the associate editor are 1) Initial handling and 
selecting reviews for the papers and 2) obtaining a decision for a paper. 
Although selecting reviewers to review a manuscript is a responsibility of the 
associate editor, it is not uncommon for an author to suggest reviewers. 
Sometimes he even needs to provide reviewers.

After getting the results from the reviewers, the associate editor decides if 
the manuscript needs to be adjusted. Formally the reviewers give an advise, but 
it is up to the associate editor to do the final judgement. If adjustments are 
needed, the author adjusts his manuscript and this needs to be reviewed again. 
This can happen with other reviewers. An alternative path is that an author 
chooses to withdraw his paper.

Eventually, hopefully, if the paper is accepted by the associate editor, an 
advise will be given to the editor-in-chief. Formally the editor-in-chief 
decides if a paper is being published, but most of the time the advise of the 
associate editor is adopted. This chain of advise and assignment is shown in 
Figure~\ref{fig:c2013}. Interesting in this schematic overview is the absolute 
power of the Editor-in-Chief. He can make a judgement without the advise of the 
associate-editor (and reviewers). Also this role can influence the process by 
choosing a specific associate-editor. Under the Editor-in-Chief, we draw the 
Associate editor. This role has two ways to influence the process; by choosing 
the reviewers and by giving the advise to the editor-in-chief. The reviewers 
have less power; off course they can provide an advise, but most of the cases 
there are multiple reviewers.

\begin{figure}[H]
\centering
\includegraphics[width=3cm]{images/c2013.drawio.png}
\caption{Roles within the journal process (visual interpretation of description in~\cite{C2013})}
\label{fig:c2013}
\end{figure}
% \outline{
% wordt zo en zo laten zien in in BH
% }
% ==============================================================================
To visualise the publication process for journals, we can use a model of 
Bj\"ork and Hedlund. They modelled the various processes involved in scientific
publishing to investigate the business impact of the shift towards internet
publishing models. This model is based on publication for journals.
For our research, we took a diagram from their publication and took the 
activities usable for our research. Hereby neglecting the steps 
"Negotiate copyright" and "Copyedit Article". The result is shown in 
Figure~\ref{fig:bh2004_a22331}.


\begin{figure}[H]
\centering
\includegraphics[width=14cm]{images/bh2004_dia_a22331_part.drawio.png}
\caption{Journal process flow between submission and decision (adapted
from~\cite[diagram A22331]{BH2004})}
\label{fig:bh2004_a22331}
\end{figure}
% ==============================================================================
The phase "Manage the review process" from Bj\"ork and Hedlund is described by 
Cormode from the perspective of the Editor-in-Chief and Associate editor (Bjork 
and Hedlund decided to use one role: editor). 



\todo{blinds, double blinds om persoonlijke connecties te ondervangen}

% ==============================================================================
\section{Conference publication}
The following is an informal description of the process for conference 
publication. This can also be a workshop or symposium. Not all conferences 
follow this approach.

For established venues the steering committee decides to have a conference and 
choose a location. The main responsibility of the steering committee is to 
oversee the organisation of the conference and choosing the program 
committee (PC) chairs which responsibility is to compose a program out of a 
subset of the received 
submissions. For new subjects, the trigger comes from one or more 
academics who thinks a conference is needed for a certain subject and try to 
form a Program Committee (PC). Possibly there are ``program tracks'' on specific 
subjects, with dedicated chairs for each track.
% ============================================================================== 
The PC chairs invite people to become program 
committee members. A new trend is self-nomination; one can nominate himself for 
a position in the program committee. Another new trend is that authors of 
submitted papers are required to review.

Besides inviting members, the PC chairs ensure the website is up, promotion 
is in place and have submission/review server set up.
The PC chairs create and distribute a call-for-papers, which starts the 
\textbf{submission phase}.
% ============================================================================== 
Responding to the call-for-papers, scientists submit papers, and possibly 
marking conflict of interest. 
% Likely, PC Chairs extend deadline, which gives 
% more scientists the possibility to submit papers. 
After a likely extension of the deadline, PC Chairs close submission and 
perform desk reject.
% ============================================================================== 

After the submission phase closes and the \textbf{bidding phase} begins. During 
this bidding phase, PC members bid on which papers they would like to review.

In the \textbf{review phase} the PC members read and score the papers. In this 
phase a paper can be early rejected with limited number of reviewers. Sometimes 
this phase contains a rebuttal phase; the reviews are send to authors so the 
authors can reply on the given comments.

In the following \textbf{discussion phase} the PC member unify their views about 
the papers.

During the \textbf{decision phase} the PC Chairs formalise a decision about a 
paper (formally; often they simply follow the consensus view). Possible 
decisions are accept, reject or conditionally accept (shepherding). In this 
case, the paper is interesting but not yet quite acceptable, salvageable. One 
reviewer is designated Shepherd and communicates with the authors on what 
changes are needed. Final acceptance hinges on the Shepherd officially 
approving the paper.

In the \textbf{camera-ready phase} the authors make final changes, 
incorporating editorial changes, reviewer comments, etc., and submit the final 
version for publication.

In the \textbf{publication phase} the PC chairs judge on the the camera-ready 
version, and papers is published online.
% ============================================================================== 


% \outline{
% \begin{itemize}
%     \item wat is macht program committee vs program chair; 
%     \item Conferentie, core ranking: https://www.core.edu.au/conference-portal
    
% \end{itemize}
% }

% \newpage
% \subsection{Model of proceedings}
% \begin{figure}[htbp]
%   \begin{center}
%         \begin{tikzpicture}[
%             auto,node distance=5cm,thick,
%             rolenode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=1.05cm},
%             thingnode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=1.05cm},
%             ]
%             % Nodes
%             \node[rolenode] (sponsoring_organization) {SO};
%             \node[rolenode] (general_chair) [below=of sponsoring_organization] {GCh};
%             \node[rolenode] (general_committee) [below=of general_chair] {GCo};
%             \node[rolenode] (program_chair) [below=of general_committee] {PCh};
%             \node[rolenode] (program_committee) [below=of program_chair] {PCo};
%             \node[rolenode] (reviewer) [right=of program_committee] {R};
%             \node[rolenode] (steering_committee) [right=of general_chair] {SC};
            
            
%             \node[thingnode] (conference) [right=of general_committee] {Conference};
%             \node[thingnode] (proceeding) [below=of conference] {Proceeding};
%             \node[thingnode] (paper) [below=of proceeding, right=of reviewer] {Paper};
%             \node[rolenode] (author) [right=of proceeding] {A};

            
%             % Lines
%             \draw[->] (sponsoring_organization) -- node [text width=2.5cm,midway,above,sloped] {appoints} (general_chair);
%             \draw[->] (general_committee) -- node [text width=2.5cm,midway,above,sloped] {organizes} (conference);
%             \draw[->] (general_chair) -- node [midway,above,sloped] {assembles} (general_committee);
%             \draw[->] (general_committee) -- node [midway,above,sloped] {contains / advises for members} (program_chair);
%             \draw[->] (program_chair) -- node [text width=2.5cm,midway,above,sloped,align=center] {assembles} (program_committee);
%             \draw[->] (program_committee) -- node [text width=2.5cm,midway,above,align=center] {assigns} (reviewer);
%             \draw[->] (conference) -- node [text width=2.5cm,midway,above,sloped] {leads to} (proceeding);
%             \draw[->] (proceeding) -- node [text width=2.5cm,midway,above,sloped] {contains} (paper);
%             \draw[->] (reviewer) -- node [text width=2.5cm,midway,above,sloped] {reviews} (paper);
%             \draw[->] (author) -- node [text width=2.5cm,midway,above,sloped] {writes} (paper);
%             \draw[->] (steering_committee) -- node [text width=2.5cm,midway,above,sloped] {plans} (conference);

%         \end{tikzpicture}
%   \end{center}
%   \caption{Publication process for collections.}
%   \label{fig:structure}
% \end{figure}

% ==============================================================================
\section{Fraud in the publication process}
\label{sec:domain_analysis:fraud}
% \outline{
% \begin{itemize}
%     \item Domain
%     \item Impact van process Welk effect heeft publiceren
%     - \# publicaties
%     - \# citaties
%     - kwaliteit
%     \item beter doen levert voordelen
%     \item dus zijn er incentives voor valsspelen
%     \item Idee\"en egregious examples -> als deze situatie voorkomt, wil ik 
%     het vinden
% \end{itemize}
% }
% ==============================================================================
In the introduction we already referred to the aphorism known as Goodhart’s Law 
: ``When a measure becomes a 
target, it ceases to be a good measure''~\cite{strathern_1997}. Known metrics 
such as the H-Index not only incentivise scientific 
excellence; they also invite other ways to achieve high rankings (e.g. 
plagiarism, manipulation of images, manipulation of research data, 
fabrication of research data). 



% \todo{Naar introductie? > is al}
% The underlying assumption is
% that the various types of scientific fraud all share a common characteristic: 
% their goal is the same, that is, their goal is to improve a researcher’s or 
% publication’s quantitative measures of science. This elevates them above their 
% peers – which on the one hand brings recognition and accolades, but on the other, 
% it makes them stand out. In short, the underlying
% assumption is that fraudsters aim to become outliers (in terms of quantitative measures of
% science).


% \outline{
% \begin{itemize}
%     \item probleem -> valsspelen
%     \item - wat is dat
%     \item - op oneigenlijke gronden
% \end{itemize}
% }
% ==============================================================================
% Examples already mentioned in the introduction are plagiarism, manipulation of 
% images (e.g., of cell 
% extracts in life science publications), manipulation of research data, 
% fabrication of research data. In addition, there may be as-of-yet unknown forms 
% of fraud.

% \outline{
% \begin{itemize}
%     \item dan interessante gevallen
% \end{itemize}
% }
% The this section we elaborate on an interesting case that may contain fraud.
% Furthermore, during the case studies we will provide some other examples.
In this section provides short description of some interesting cases that may 
occur.
It is certainly not the case that all situations do actually imply fraud, but 
these are cases that are a candidate for further manual investigation.

\paragraph{Publications cites publications in the same venue} It is remarkable
if a venue cites only work from itself. The benefit for this venue is that it
increases the impact score. This case is described as an example in 
Section~\ref{interesting_case:publications_cites_publications_in_same_venue}.
% ==============================================================================
\paragraph{Work member of editorial board is being cited} Member of the
editorial board that is excessively often being cited in his own venue, can be
an indicator for fraudulent behaviour. This case serves as input for case 
study~1 (Chapter~\ref{chp:case1}) and is described further in 
Section~\ref{interesting_case:work_member_editorial_board_cited}.
% ==============================================================================
\paragraph{Member of the editorial board is (co-)author} 
A questionable situation occurs if a member of the editorial board becomes co-
author of \textit{relatively} a lot of publications. This may indicate that 
authors are being forced to add the member to the author list. The benefit for
the member increasing his publication count and, if the article gets cited, 
citation count (which has impact on the H-Index and therefore impacts his 
career).

This situation is described in case study 2
(Section~\ref{interesting_case:member_editorial_board_is_coauthor}).

% ==============================================================================
\paragraph{Author reviews his own work}
This kind of fraud is known because of Moon. Moon gave an e-mail address of 
himself (alias) as reviewer for his own work. This case was detected because of
the short time between submission and accordance. Moon was detected, but are 
other authors that show this fraudulent behaviour also detectable?
Case study 3 (Chapter~\ref{chp:case3} is based on the publication-lag (time 
between submission and publication of an article).

\section{Problem statement}
\label{sec:problem_statement}
There are processes to publish articles for journals and conferences.
In these processes fraud is conducted.
To improve detection of this fraud, data is necessary.
Previous approach (\cite{TEJ2017}) to discover fraudulent behaviour was based 
on public datasets, but misses important entities. 

Considering the cases described in Section~\ref{sec:domain_analysis:fraud} and 
public datasets that are generally represented by Jonker and Maauw~\cite{JM2017},
missing entities can be pointed out.
In Figure~\ref{fig:jm2017_missing_ent} the generic data model for public 
sources is on the right side; V for venue, P for paper and A for author. Left of 
this model missing entities are added.

\begin{figure}[H]
\centering
\includegraphics[width=7cm]{images/jm2017_miss_ent.png}
\caption{Right: Set-theoretic publication model adapted from~\cite{JM2017} Left: Missing entities considering example attacks.}
\label{fig:jm2017_missing_ent}
\end{figure}

However, these missing entities are available and `human'-readable, but not 
integrated in existing datasets.



% \section{Interesting cases}
% \label{interesting_cases}
% In this section we describe theoretical and practical (actually happened) 
% attacks on the integrity of the publication process. We can use these attacks to 
% complete the theoretical (or logical) publication model.



% \outline{
% \begin{itemize}
%     \item The articles of a journal that are cited impact the journal impact factor.
%     \item It is important that this number is high.
%     \item Meetbaar door de verhouding te bekijken van de journals waar de geciteerde artikelen uit komen.
%     \item \todo{Wie is dan interessant? author of editor?}
%     \item In praktijk al Thomson
%     \item Verwachting is niet veel op vinden
%     \item deze laten zitten
%     \item als voorbeeld hoe de modellering werkt
% \end{itemize}
% }


% \subsection{All time favourite: Diederik Stapel}
% % ------------------------------------------------------------------------------
% \vraag{Wat met Diederik Stapel?}
% \outline{ergens in het begin: er zijn manieren om te frauderen...}

% The fraud that Diederik Stapel committed was not related to the publication 
% process, but to the data he used to draw conclusion on.
% With our specific method, we will not be able to detect data manupulation and
% fabrication.
% However the \textit{effect} of his approach is a lot of publications.

% \outline{door neven effect van fraude (bijvoorbeeld heel veel publicaties) of 
% zijn resultatenn halen het nieuws / opgemerkt in academische wereld omdat ze 
% fout zijn / omdat heel veel citaties eerste maanden zijn}.

% ------------------------------------------------------------------------------

% \outline{
% \section{placeholder}

% \begin{itemize}
%     \item Author and reviewer are the same person: $\{h \in \Humans \mid \authors(p, h) \land \reviews(p, h)\}$ (Moon)
%     \item Editor-in-Chief is part of authors: $\{h \in \Humans \mid \authors(p, h) \land \receives(p, h)\}$. Logisch gevolg is dus dat $\accepts(p, h)$
%     \item Citaat naar paper van iemand in het proces: stel q is de publicatie waar het om draait dan:
%     $\{h \in \Editors \cup \Reviewers \mid \cites(q, p) \land \authors(h, p)\}$
%     \item bovengemiddeld zelfcitaties
%     \item publicaties die geciteerd worden komen uit dezelfde journal\todo{???}
% \end{itemize}
% }

% \outline{
% \begin{itemize}
%     \item dan modellering (hfst \ref{chp:modelling_the_publication_process})
% \end{itemize}
% }

% ==============================================================================
\chapter{Related work}
\label{chp:related_work}
% ==============================================================================
\outline{
Bijdrage aan contributies
\begin{itemize}
    \item 1) Studies die iets soortgelijks hebben gedaan
    \begin{itemize}
        \item Tielenburg
        \item This work heavely relies on the work by Tielenburg~\cite{TEJ2017}.
        \item T uses two steps:
        \begin{itemize}
            \item 1) outlier detection phase
            \item 2) peer comparison phase
        \end{itemize}
    \end{itemize}
    \item 2) Bouwstenen
    \begin{itemize}
        \item PDF Extractie
        \item Outlier detectie
    \end{itemize}
    \item 3) Titels die altijd moeten worden genoemd
\end{itemize}
}


% ==============================================================================
\chapter{Methodology}
\label{chp:methodology}
% ==============================================================================

Chapter~\ref{chp:domainanalysis} ends with the available but not integrated 
entities to enrich datasets about the scientific publication process.

In this research we want to investigate 
how integrated public data coming from different sources improves the detection 
of scientific fraud.

\paragraph{Define datamodel}
In the problem statement (Section~\ref{sec:problem_statement}) we mention some 
missing entities. The resulting figure is not sufficient to build a dataset 
to detect fraud. Therefore, as first step we need a more concise datamodel to 
enable detection of interesting cases. Our goal with this step is to identify
entities and their relations.
Possible approaches to get to a datamodel:
\begin{description}
    \item[1. Use an existing datamodel] Datamodels about the publication process
    do exist.
    \item[2. Create a model] In this case we need to build our own model. The 
    description domain analysis can be used an input.
\end{description}

We choose a combination of these approaches. The reason is that we found 
existing models are all somehow limited to 
the author, venue and publication. Attacks as described in 
Section~\ref{sec:domain_analysis:fraud} are not possible to detect in 
these models. As an example: the Microsoft Acadamic Graph 
model\footnote{\url{https://docs.microsoft.com/en-us/academic-services/graph/reference-data-schema}} 
(which is the most detailed we came across) does not contain any other role 
than author.
By using a more high-level logical model approach, we can define a model 
focused on the entities and relations involved, which can be implemented 
using multiple physical modelling techniques. 

The second step is to determine which publicly available dataset can fulfill 
some parts of the model defined in the first step. Out appraoch is here to 
discuss the available datasets and how these covers the model. Also, how 
can we integrate these datasets to create a complete view of the model.



% ============================================================================
\paragraph{Acquiring and integrate data}
Our approach is to execute case studies where we acquire and integrate 
public sources into the model from the previous step. These case studies are 
based on cases described in Section~\ref{sec:domain_analysis:fraud}.

Alternitive approach would be to acquire data, create one integrated dataset, 
followed by applying outlier analysis on this dataset. 
But there are a few reasons not to do this:
\begin{itemize}
    \item 1) Not all data is available of all entities. Some data is hard to 
    acquire and not generally available. We need to
    execute a case study on a certain group where this data is available for.
    As we only consider the group where we were able to get the data for, a 
    more fair compare is done. Our purpose is to demonstrate that it is 
    rewarding to integrate datasets; not to get this data for the whole population.
    \item 2) Our defined attacks are all 'related to'; compared within a group of
    peers. This also implies the need for a group based approach (e.g. group 
    of editors).
    \item 3) Tielemans tried to apply outlier detection on the whole population, 
    without succes. This does not mean his approach will not work with integrated 
    additional data. However his study we take a step back; add additional data 
    for a group and perform outlier analysis within this group. An approach 
    involving the whole set is probably too soon; first investigate if additional 
    data improves the detection (this study), then acquiring the data for the 
    complete set and maybe then a research can be conducted on applying outlier 
    analysis on the whole set (alternative approach).
\end{itemize}

\paragraph{Case studies}
In the domain analysis of the publication process interesting situations are 
described with the groups having the power to abuse that particular situation
(Section~\ref{sec:domain_analysis:fraud}). In Figure~\ref{fig:research_method} this
is these are represented with the arrows from the publication process to the roles.

\begin{figure}[H]
\centering
\includegraphics[width=13cm]{images/research_method/research_method.png}
\caption{Relation publication process, roles and case studies}
\label{fig:research_method}
\end{figure}
Based on these situations three cases studies (identifiable groups) are formulated:
\begin{itemize}
    \item Program Committee Members
    \item Editors of a journal
    \item Authors \todo{articles?}
\end{itemize}

For every use case we need to address a few items:
\begin{itemize}
    \item Acquiring the additional data
    \item Create an analysis dataset from the publicly available data with the 
    newly acquired data.
    \item Validate possible interesting cases.
\end{itemize}

% To answer the question (how can we identify possible fraudulent 
% behaviour in a specific group of peers) involves a structured approach.
% First we need to formulate a dataset to perfrom the analysis on. At forehand
% we do not know if the datasets available from public sources are sufficient enough.

% During these case studies we will need to answer some questions:
% Why is this an interesting group to analyse?
% Is the dataset from RQ1 sufficient to analyse this group?
% If we need additional data, what is the availability of this data?
% And how can we integrate this data to perform outlier detection.

\paragraph{Formalising the conclusing}
The combinations of validations from the case studies will result in the
answer on the question if acquiring and integrate publicly data improves
the detectablility of scientific fraud.


\outline{
Belangrijkste punten:
\begin{itemize}
    \item Wat ga je doen?
    
    We will do this by:
    - Creating a model of the publication process that is extensible with other data related to the publication model and investigate how we can fill this with existing datasets.
    - 
    
    
    Approaches for validation:
    \paragraph{Peer comparison} This approach contains comparing the `group' with its peers.
    
    \paragraph{Derived metrics} By calculating metrics of a certain group we could run an outlier detection.
    
    \paragraph{} We choose the first option. The reasons are:
    \begin{itemize}
        \item We want to stay as close to the raw data as possible. Defining derived metrics and apply outlier detection based on these, has been performed by Tielenburg without success.
        \item 
    \end{itemize}
    
    Validation will be executed for every case. Here we will consider the added
    data as group and determine if this results in outliers.

    
    \item Waarom ga je dat zo doen?
    \item er zijn andere manieren: ik kies deze methode want...
    \item wordt nu gekeken naar input kant -> willen naar effect kijken (output kant, onafhankelijk van input kant)
    \item tielenburg heeft gekeken groter trekken -> werkt niet, data niet rijk genoeg
    \item Draait om detecteren of er aanvallen plaats vinden
    \item Wat hebben we nodig om aanvallen te detecteren (hoe kunnen wij dat 
    doen?
    \item Redenering dat we een model nodig hebben
\end{itemize}
}

% ==============================================================================
\section{Research Questions}

% Hoofdvraag: Heeft outlier detectie binnen een groep zin?
The main question we want to answer in this research is: \\
\textbf{What is the impact of using a group-based outlier detection approach in 
detecting fraud in the scientific publication process?}

What we want to validate in this research is: \\
\textit{There exists fraud in the publication process which causes fraudsters
to become detectable outliers with respect to their peers.}

\todo{of deze (uit de introductie):}
The statement we want to verify with this study is:
People with malicious ways to accomplish improvement of their quantitative 
measures, will be an outlier compared with their peers.

Hypothese: Door breder trekken van data zijn we wel in staat om outliers te detecteren

Vraag: waar schiet het model te kort
ant: in praktijk deze aanvallen -> deze data nodig -> niet beschikbaar in databronnen

identificeren attributen
data bronnen
acquisitie
integratie

We define the following subquestions:

\begin{enumerate}[label=RQ\arabic*]

    \item \label{RQ1} \textit{How can we define a dataset from known public 
    datasources to populate a formal publication model?}
    \item \label{RQ2} \textit{How can we identify possible fraudulent behaviour
    in a specific group of peers?}

\end{enumerate}

% -----------------------------------------------------------------------------------
\subsection{Research method}
\label{Research_ResearchMethod}
In this section we elaborate on how we are going to answer the questions.

\subsubsection*{RQ1: Data model and acquisition}
This subquestion is twofold: first a definition of the formal publication datamodel, 
and second how we can load this dataset with existing sources.

To support outlier detection, we need a datamodel of the publication process. This 
data model is needed to provide a framework where to place the necessary objects and 
attributes and define the relationships between these objects.

After we have this model we will discuss some public available datasets in the area 
of computer science and how we can use these sets to load the data model. 

We will judge this step by completeness and integratability of the datasets.

% -----------------------------------------------------------------------------------




% ==============================================================================
\chapter{Data model and Acquisition}
\label{chp:data}
% ==============================================================================
In this chapter we describe the underlying data model to perform group based 
outlier detection. This chapter addresses three point:
\begin{itemize}
    \item The ideal data model for the publication process;
    \item Known data sources to fill this data model;
    \item Completeness of these known sources to fill the data model.
\end{itemize}


\section{Ideal data model}
By formalising the ideal data model, we can answer which entities and which 
roles are involved and how are they related.
The ideal situation is one dataset which contains all entities and relations 
and can serve as base to easily create the necessary datasets upon to perform 
the data analysis. As 
input to reason about this dataset we use the domain analysis given in 
Chapter~\ref{chp:domainanalysis}.

As starting point; Jonker and Mauw formalised the publication model~\cite{JM2017} 
and created an entity based view of the publication process. 
Figure~\ref{fig:jm2017_induced_pub_model} depicts their final model.
\begin{figure}[H]
\centering
\includegraphics[width=3cm]{images/jm2017_undiced_pub_view.drawio.png}
\caption{Set-theoretic publication model of papers (P), venues (V), and 
authors (A) (adapted from~\cite{JM2017})}
\label{fig:jm2017_induced_pub_model}
\end{figure}
% In this figure, the V stands for venue, P for publication and A for author. 
% Based on this model, research is conducted to identify if interesting authors 
% can be identified based on metrics derived from this model. One such research 
% is the work of Tielenburg~\cite{TEJ2017}. Unfortunately, the results where not 
% sufficient to provide a solid framework to detect interesting authors.

% ==============================================================================

Whereas the Jonker and Mauw only modelled the author as a role in the process, 
Bj\"ork and Hedlund mention the publisher, editor and reviewer as well. 
We consider the author from Jonker and Mauw the same as the researcher from 
Bj\"ork and Hedlund. In this research, we ignore the publisher. Although the 
publisher is responsible in the end and sets the regulations and constraints for 
editors to work within, the actual work is delegated to editors.

% \todo{Hier moet nog iets meer. Bijvoorbeeld: https://www.councilscienceeditors.org/resource-library/editorial-policies/white-paper-on-publication-ethics/2-1-editor-roles-and-responsibilities/}


% Unfortunately, Bjork and Hedmung lacks the opportunity to describe the process. However, we can get some textual information from Cormode from his work as an associate editor~\cite{C2013}.

% Cormode also focusses on the journal publication. We will gather the roles and responsibilities from his article and use this to complete our perspective.
% % ==============================================================================
% \paragraph{Editorial Board} Cormode describes that the structure can vary depending per journal. In most cases, generic structure for a journal is that there is a editorial board. This board consist of the Editor-in-Chief and multiple Associate Editors. The main role of this board is to determine which papers to accept.
% % ==============================================================================
% \paragraph{Editor-in-chief} This role decides which papers to include. The information to make this decision comes from the Associate Editor. This role receives the actual papers and delegate the work to associate editors.

% % ==============================================================================
% \paragraph{What about the author}
% Cormode also gives suggestions to certain roles in the process. In our case, the suggestions he makes for an author are important: 1) Suggest suitable Associate Editor and 2) Suggest suitable Reviewer. If the editor-in-chief and associate editor make use of this suggestion, the author can gain power in the judgement of his own work.



% \outline{
% \begin{itemize}
%     \item Reviews
%     \item vaak meerdere
%     \item daarom minder risico op fraude???
%     \item ali \& watson bekijken
% \end{itemize}
% }

% ==============================================================================
% \section{Extending the model}
We can extend the model of Jonker and Mauw with the description of Cormode to 
come up 
with a model that incorporates the most important roles with influence on the 
judgement of the work of an author, which results in judgement of the production 
of the author, which eventual lead to possible benefits for the author. We 
ignore roles like the editorial assistance and subeditor, because these roles 
does not seem to have influence on the judgement of a manuscript. In 
Figure~\ref{fig:structure_journal} we provide an overview of the entities and 
roles from Jonker and Maauw combined with the information from Cormode which 
answers which roles are involved and how they are related.

The red rectangles are objects and the green circles are roles.
As objects we have a venue which contains papers. Multiple roles are related to these objects.
% We define the following sets:
% \begin{itemize}
%     \item Authors: $A(p) = \{h \in H \mid \authors(p, h)\}$
%     \item Reviewers: $R(p) = \{h \in H \mid \reviews(p, h)\}$
% \end{itemize}


% ==============================================================================

\begin{figure}[H]
  \begin{center}
  \small
        \begin{tikzpicture}[
            auto,node distance=2cm,thick,
            rolenode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=1.05cm},
            thingnode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=1.05cm},
            ]
            % Nodes
            \node[rolenode] (eic) {EiC};
            \node[rolenode] (ae) [below=of eic] {AE};
            \node[rolenode] (reviewer) [below=of ae] {R};
            
            \node[thingnode] (venue) [left=of eic] {V};
            \node[thingnode] (publication) [left=of reviewer] {P};
            \node[rolenode] (author) [below=of publication] {A};
            
            % Lines
            \draw[->] (publication) -- node [text width=2.5cm,midway,above,sloped] {published at} (venue);
            \draw[->] (eic) -- node [midway,above,sloped] {responsible for} (venue);
            \draw[->] (eic) -- node [midway,above,sloped] {accept / reject / receives} (publication);
            \draw[->] (reviewer) -- node [text width=2.5cm,midway,above,align=center] {reviews} (publication);
            \draw[->] (author) -- node [midway,above,sloped] {authors} (publication);
            \draw[->, transform canvas={xshift=2pt}] (eic) -- node [midway,above,sloped] {assigns} (ae);
            \draw[->, transform canvas={xshift=-2pt}] (ae) -- node [midway,above,sloped] {recommendation} (eic);
            \draw[->, transform canvas={xshift=2pt}] (ae) -- node [midway,above,sloped] {assigns} (reviewer);
            \draw[->, transform canvas={xshift=-2pt}] (reviewer) -- node [midway,above,sloped] {feedback} (ae);
            
            \path (publication) edge [loop left] node {cites} (publication);
            
            % \draw[->] (publication.west) .. controls +(down:7mm) and +(left:5cm) .. (publication.west);
        \end{tikzpicture}
  \end{center}
  \caption{Set-theoretic model of the publication process for collections.}
  \label{fig:structure_journal}
\end{figure}

It is important to notice that the green circles are roles, not entities. These 
roles are fulfilled by people. This results in one more entity (person) with 
relation to other entities. This relationsips can be expressed as the roles 
(e.g. a person is a reviewer for a publication).

% ==============================================================================
\subsection{What about conferences?}
So far we only focused on the process for journals. But, as mentioned before, 
for computer science, publishing for a conference is a big deal.

In case of conference publications, the author, publication and its relation to 
the venue stays the same. It can be that this venue becomes more extensive 
because of tracks, but it stays a venue.
The important part are the Program Committee Members. However, this is this a 
person that has a certain role in relation to the venue.

% ==============================================================================
\subsection{Resulting model}
In the resulting model we choose to abstract the roles away and use one entity 
called person where we define the roles to certain entities in the relationship. 
This makes the model far more easy to reason about when a person has multiple 
roles.

The model in Figure~\ref{fig:resulting_model} is not a physical model. The 
physical model can in implemented in multiple design strategies like Ensemble 
Modelling (Data Vault or Anchor modelling). Which modeling principle is being 
applied is not important, what is important, is that the physical model needs to 
be aware of time and changes to the entity to create a physical model that is 
able to give answer to the important things. E.g. a person is not 'always' 
editor of a venue, this role starts and stops at a certain moment in time.
\begin{figure}[H]
\centering
\includegraphics[width=9cm]{images/set_model.png}
\caption{Set model which covers necessary entities and relationships}
\label{fig:resulting_model}
\end{figure}

% ------------------------------------------------------------------------------
\subsection{Example: Publications cites publications in same venue}
\label{interesting_case:publications_cites_publications_in_same_venue}
In this part we will go through a situation to describe the process of analysing
a situation which can be an attack on the publication process. The chance this 
particular situation occurs is not very high and we will
not incorporate this in our model. The reason for this is that there is a good
enough safety net to catch this situations provided by Thomson, institute that 
calculates the Journal Impact Score. Besides, in our research we are looking for
interesting persons, and we do not consider a venue a person (although in the
end someone must be responsible).

\textit{First we describe the situation and provide a model of instances that 
are involved. This can be persons in a certain role, venues, publication etc}.

Having publications citing publications from a certain venue, increases the 
journal impact factor.

We give a model of this situation in Figure~\ref{fig:cpsv}.

\begin{figure}[H]
\centering
\includegraphics[width=9cm]{images/cited_publications_same_journal.drawio.png}
\caption{Instance model of citation of publication in the same venue}
\label{fig:cpsv}
\end{figure}

In Figure~\ref{fig:cpsv} we see a venue (v1) that conains a publication (p1)
which has a citation (c1) to a publication (p2) in the same venue.
% ------------------------------------------------------------------------------
\paragraph{Attack}
\textit{We describe when (this is not always the case, see 'Beneign 
alternatives') and how this situation is an attack.} 

It is not unusual that 
publications in a journal cite publications from the same venue. However, as we 
will see with most attacks, when this occurs much more 
compared with other venues and the 'self'-citation rate is higher, it becomes 
interesting. This may indicate that this is a conscientious action to increase
the Journal Impact Score, and that an attack.

\paragraph{Model impact}
\textit{In this part we describe how this situation impacts the publication 
model.}

What does in- or decrease and on what way this impacts the 'score' (or index) 
of the attacker.
The impact on the publication model is that the number of citations to 
publications in the same venue impact score of a journal raises. This number has
impact on the journal impact score.

\paragraph{Beneign alternatives}
\textit{In this part we describe how this situation can occur, while not being
an attack.} 

If a certain venue is that specialised in a certain topic, the 
situation where a publications cites other publications in that venue can occur.



% \section{Main source}


% \subsection{Data acquisition}


% \outline{
% % \begin{itemize}
% %     \item Question to answer: Which data do we need and how can we get it?
% %     \item om te weten welke data we nodig hebben kijken we naar het publicatie process -> beschreven in voorgaand hoofdstuk
% %     \item vervolgens kijken we naar de data die beschikbaar is in de publiek beschikbare databronnen
% %     \item mismatch
% %     \begin{itemize}
% %         \item welke informatie missen we
% %         \item hoe kunnen we hier aan komen
% %     \end{itemize}
% %     \item ophalen van mismatch
% % \end{itemize}

% }

% ==============================================================================
\section{Public available datasets}
In this section we will explore some publicly available datasets with the focus 
on Computer Science which can be use to fill the model from the previous 
section. Because of the necessary ability to integrate datasets, we
will judge these sets on integrability and completeness.

As a guideline, we will use Figure~\ref{fig:dataflow_jm2017_2}.
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{images/data_to_publication_metrics_jm2017.png}
    \caption{Data flow (\cite{JM2017})}
    \label{fig:dataflow_jm2017_2}
\end{figure}

The sigma-sign represents the filter on the set. For now, this filter sets the 
focus on the Computer Science area. Later on, this filter can be more 
strict, depending on the case study.

One datasets is known which focus solely on Computer Science; DBLP.
For other research areas, other datasources should be taken (e.g. for biomedical 
research, one can use PubMed).

% \begin{itemize}
%     \item Publicly available sources are limited in available data. Most datasets provides information on who wrote what and sometimes which article refers what. This provides a good insight and based on these datasets a lot of research is already conducted.
%     \item However, we think these datasets miss actors on the publication process that are in a position with enough power to influence the publication process.
%     \item Data about these people involved in the publication process are not structured available to use in analysis. The information is provided in front matters and masthead documents. Because of the nature of these documents, mostly PDF's, we consider this as unstructured data.
%     \item We are somehow surprised of the lack of structured data availability about these people, because of their power and influence.
%     \item Our focus in this chapter is to incorporate this less structured available data into publicly available data about the publication process to get a dataset which provides a more complete view of the publication process.
%     \item Our research focus on information technology so our start point to gather these people is to use the most prominent conferences and journals. As source for this data we use the core ranking.
% \end{itemize}

% \outline{
% \begin{itemize}
%     \item Model wat in dblp, aminer en in HJ staat
%     \item Model publicatieprocess
%     \item Sluit niet aan
%     \item meer rollen -> niet in publieke datasets
%     \item Most public available datasources for the publicationprocess contain information about the author and what the author publishes. Sometimes (but not all, or incomplete) these datasets also contain references.
%     \item The model of these datasets are visualized by the publication process of Jonker and Mauw.
%     \item Examples of these datasets are DBLP and Aminer.
% \end{itemize}
% }


% ==============================================================================
\subsection{DBLP}
DBLP is a dataset with bibliographic information in the computer science 
discipline which can be downloaded in XML format. In 
Figure~\ref{fig:set_model_dblp} we show which elements of the set are 
covered by DBLP.

\begin{figure}[H]
\includegraphics[width=9cm]{images/set_model_dblp.png}
\centering
\caption{Set model DBLP}
\label{fig:set_model_dblp}
\end{figure}

% \outline{
% \begin{itemize}
%     \item Structure of delivery
%     \item Processing
%     \begin{itemize}
%         \item convert XML to relational tables
%         \item Every bibliographic type in separate table
%         \item Assign unique id
%         \item Make separate table for elements with id of bibliographic record
%         \item Attributes of elements are columns in table
%         \item Add metadata to all data entries
%     \end{itemize}
%     \item Result
%     \begin{itemize}
%         \item 35 tables
%         \item overzicht van tabellen en relaties
%     \end{itemize}
% \end{itemize}    
% }

Unfortunate DBLP misses some important information to be complete; the dataset
does not contain references (yet), editor and pc members and reviewers. For 
references, other sources can be addressed: Aminer and OpenCitations.

% ==============================================================================
\subsection{Aminer}
Aminer positions itself as an AI for academic publications. One dataset they 
provide is the 
'Citation Network Dataset'. This dataset combines data from DBLP, ACM (a 
publisher), MAG (Microsoft Acaedmic Graph)
and other sources. Becauses it uses DBLP as a source, it is interesting to 
investigate if we can use this for filling the gap of the citations.
In Figure~\ref{fig:set_model_aminer} we presented which entities and roles
from our ideal data model this dataset contains. Although this set contains data
from these entities, the attributes are limited. This set was created for
citations, so other entities are not filled in with valuable attributes.
\begin{figure}[H]
\includegraphics[width=9cm]{images/set_model_aminer.png}
\centering
\caption{Set model Aminer}
\label{fig:set_model_aminer}
\end{figure}


\subsection{OpenCitation} 
OpenCitation is a DOI based reference dataset. This means, the cited and citing 
articles are referred to by a doi. 
The dataset from opencitation is can be downloaded, or questioned by an API request. 


\subsection{Google Scholar}
\label{subsec:google_scholar}
Also Google Scholar contains the references from and to articles. This data is 
only available online. Some software packages exists that support scraping this
information. However, experiments from the research preparation phase concluded
that this was not a feasible solution because of the amount of captcha's when
trying to scrape the site. The acquisition of data is not practical.

\subsection{Conclusion data availability}
\label{subsec:ConclusionsDataAvailablity}
Referring to the ideal model as given in Figure~\ref{fig:resulting_model}, we 
can make a few statements:
\begin{itemize}
    \item The availability of data about publications, venues and authors is 
    good; almost all public sources contains some information about these 
    entities. 
    \item References are available in two public available sources (we ignore 
    Google Scholar for given reasons in Subsection~\ref{subsec:google_scholar}).
    \item Other roles than `author' are not available in public datasets.
\end{itemize}

In the next section we will investigate how these sources can be integrated.

% \section{Deviation publication model and available data}
% As visualised in Figure~\ref{fig:set_model_dblp} and Figure~\ref{fig:set_model_aminer}, both datasets miss some crucial roles.
% Both sources miss the reviewer, editor and PC Member. These roles are crucial in the publication process and are a must-haves in our group-based outlier detection.

% This raises the question where we are able to get this data. Luckily, there are is some data about these roles publicly available, but this is not straight forward. We will come back to this point when we describe the case studies.

% ==============================================================================
\section{Integrability}
People, articles and venues can be acquired from multiple source. To be able to 
work with these entities, we need to integrate these sources in one model. To 
make this possible, we need to uniquely identify the entities.

Again referring to Figure~\ref{fig:dataflow_jm2017_2}, we can consider this the 
tilde sign.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{images/data_to_publication_metrics_jm2017.png}
    \caption{Data flow (\cite{JM2017})}
    \label{fig:dataflow_jm2017_2}
\end{figure}

In the ideal situation is that entities have an unique identifier which is 
independent of the source. However, in practice it is a bit more complex than 
that.

% ==============================================================================
\subsection{Articles}
The unique key which is used to identity an article is the Digital Object 
Identifier. This field is available in DBLP, Aminer and OpenCitations. DOI 
should be unique across all articles. Theoretical, this should be easy going.

However, an analysis of the DBLP data set leaves us with a few DOI's which refer
to multiple articles. The number of articles this occurs, is very low. Also, DBLP 
has a lot of documents where the DOI is unknown; so data completeness 
is an issue here. \todo{Hoe veel dan? Iets van aantallen?}

\subsubsection{DOI Availability}
DBLP contains additional information about an entry in an ee tag. These contain 
links to doi.org, wikidata, etc. Sometimes an article has multiple doi.org links; 
this seems mostly the case with the publisher ACM. Although the links are 
different, they redirect to the same page at ACM.
For articles the following count of dois is available.
\begin{center}
    \begin{tabular}{ |c|c|c|c| }
        \hline
        \# of DOI & \# of articles \\
        \hline
        0 & 1051229 \\
        1 & 4626193 \\
        2 & 22396 \\
        > 3 & 1202 \\
        \hline
    \end{tabular}
\end{center}

Concluding; most articles do have a DOI; approximately 78\% has one or multiple 
doi's.

Aminer provides one doi field for an article. Also not all DOI's are available.
For the 4894081 articles in Aminer, 3920939 has a DOI (approximally 80\%).
However, the doi's provided in the dataset are not unique: 3316 DOI's are used
to identify multiple articles. A quick investigation learned us that Aminer 
sometimes uses a doi to refer to a journal instead of an article. 
Another finding with a quick analysis in the case of DOI's, is that multiple articles
are represted multiple times in the dataset; the article dataset is not unique. This 
raises some doubt about the quality of the dataset of Aminer.

\subsubsection{Alternative key}
Because the DOI is not a good key across all sources to integrate data because 
of
availability and uniqueness, we can see if there are alternatives. In this case
we need to define the fields that make a good candidate to serve as key. 
A combination of title, authors, year and journal make a good candidate.
% ==============================================================================
Let's start with the title. 

The title and how sources save this title may differ. 
Sometimes the sources uses other characters. For example, BDLP uses specific 
some kind of html tags to specify characters in the title, e.g. see 
Listing~\ref{lst:DblpTitle}. This in combination 
with specific symbols (e.g. Greek delta-sign) which are expressed differently 
depending on the source, make joining sets on these signs cumbersome.

\lstset{language=XML}
\begin{lstlisting}[caption={Example title in DBLP},label={lst:DblpTitle}]
<title>Graphs of Bounded Treewidth can be Canonized in AC<sup>1</sup>.</title>
\end{lstlisting}

However, these problems can be overcome. Still we need a way to figure out how 
to join on authors. Which raises a next integration problem: people.

% === oud, weg misschien
% However, this document id is not filled in for all titles. For
% the papers that do not have an doi, we can do the match on title. Unfortunately,
% this also is not that straightforward. The title sometimes contains latex 
% specific characters, or other weird characters.

% ==============================================================================
\subsection{People}
\label{sec:integrability_people}
In the publication domain there exists an identifier for people, this is called
the Open Researcher and Contributor ID (ORCID). This field is (partially) 
available in DBLP, but not in Aminer.

Alternative approach to uniquely identify authors is the name. However, this has
multiple issues. 
\begin{description}
    \item[Format] A name can be written in multiple formats. Say a person is 
    named Alfred Jodocus Kwak\footnote{Although the chance this is a duck 
    instead of a person, is pretty high}. This can be written as `Alfred Kwak', 
    `Alfred J. Kwak', `A. J. Kwak', `A. Kwak' and off course the full name. 
    With names that contain accents on letter, the possibilities increases.
    Also, in some countries it is used to put the family name first.
    \item[Multiple people with same name] The name does not make an author unique. The same name
    refers to other authors. 
    \item[Other names for the same person] E.g. when Robert is also called Bob.
\end{description}

The only source that acknowledge this problem is DBLP. In case of mulitple 
people with the same name, DBLP addresses this problem by suffixing the
name with a four digit code~\footnote{\url{https://dblp.org/faq/1474704.html}}.
DBLP is making progress here, but this is not entirely done yet. 
The format issue and alias issue is covered by DBLP by creating a master 
record for the author, see Listing~\ref{lst:DblpAuthorMasterRecord} as an 
example.
% ==========================================================================
\lstset{language=XML}
\begin{lstlisting}[caption={Example master record(https://dblp.org/faq/1474690.html)},label={lst:DblpAuthorMasterRecord}]
<www key="homepages/r/CJvanRijsbergen">
<author>C. J. van Rijsbergen</author>
<author>Cornelis Joost van Rijsbergen</author>
<author>Keith van Rijsbergen</author>
<title>Home Page</title>
<url>http://www.dcs.gla.ac.uk/~keith/</url>
</www>
\end{lstlisting}

However, an exploratory analysis of the DBLP data makes clear this is not 
flawless; we have noticed cases where the same master record authors one 
article multiple times. It seems that a father and son both authored this 
article (Robert and Bob, same lastname so DBLP 'chooses' to treat these names
as the same person).

But hypothetically, even if DBLP mentions to unique identity authors, 
the match to authors in other datasources is not possible, if they do 
not adhere the same method.


% \subsection{Article integration}
% Not only do we need to match the articles of DBLP and Springer LNCS, we also 
% need to incorporate Aminer. Aminer is an enrichment of DBLP in that it adds
% citations. Although DBLP is working to incorporate citations in their dataset,
% this is still work in progress and Aminer has already a lot of that data.
% \todo{Stukje over Aminer}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=10cm]{images/linking_article_dblp_lncs_aminer.drawio.png}
%     \caption{Matching Springer LNCS, DBLP and Aminer}
%     \label{fig:linking_article_dblp_lncs_aminer}
% \end{figure}


% ==============================================================================
\subsection{Conclusion integrability}
From this section we can draw the following conclusions:
\begin{itemize}
    \item Attributes that could serve as a cross-source key to integrate data 
    (orcid, doi) is not complete enough.
    \item Alternative methods to integrate the data have too many weak spots to
    be sustainable.
\end{itemize}

Combining these statements with the statements from 
Subsection~\ref{subsec:ConclusionsDataAvailablity}, we choose DBLP as root for
the dataset on the way forward.

Because of the lack of integrability we are unable to fill the citations, 
although Aminer and OpenCitations can deliver these. The result is that we 
should take a pragmatic approach when working out the case studies; see what 
data we need to incorporate, what attributes are to our disposal and make 
choices based on that information.

Next step is to actually load the data of DBLP.

% ==============================================================================
\section{Processing of DBLP dataset}
The dataset of DBLP is XML based. For every bibliograpic record an XML element 
is available. These record contains elements with additional information, e.g. 
author, title and journal. In Listing~\ref{lst:DblpExample} we show one 
bibliographic record (an article) as delivered in the dataset of DBLP.

\lstset{language=XML}
\begin{lstlisting}[caption={DBLP bibliografic record example},label={lst:DblpExample}]
<article mdate="2019-10-25" key="tr/gte/TM-0332-11-90-165" publtype="informal">
<author>Frank Manola</author>
<author>Mark F. Hornick</author>
<author>Alejandro P. Buchmann</author>
<title>Object Data Model Facilities for Multimedia Data Types.</title>
<journal>GTE Laboratories Incorporated</journal>
<volume>TM-0332-11-90-165</volume>
<month>December</month>
<year>1990</year>
<url>db/journals/gtelab/index.html#TM-0332-11-90-165</url>
</article>
\end{lstlisting}

% ==============================================================================
Possible approaches to load this dataset are:
\begin{description}
\item[Option 1: Load all elements to separate tables] This approach splits all 
the data in separate tables; it converts this semi-structured dataset in a 
structured (relational) dataset. The positive points are that we have all data 
(probably also data we do not need, but we do not know that at forehand) and it 
is a process which is easily to automate. Also, storing the dataset in a 
relational database gives the opportunity to integrate the data with other 
sources. However, this approach leads to a lot of tables, cost storage and 
requires an structure that keeps the referential integrity.
% ==============================================================================
\item[Option 2: Keep it as a file and make software to question the file] We can 
choose to not load the data at all, and create some interfacing to the file to 
be able to read the data. By taking this approach we need to know the model of 
the data at forehand and create software that gives access to the data in the 
file. Also, we are not able to easy query the data, unless we build this 
functionality; functionality which already exists in every other database. 
However, taking this approach does not need to copy the data into another 
dataset, which is positive for disk allocation.
% ==============================================================================
\item[Option 3: Split the file in separate XML's and load these in the database] 
Nowadays, databases are able to handle XML file as a native datatype. The 
drawback is that, even current relational databases support this task, the 
performance is not optimal. Also querying this data is more complex and slow 
than when all data is relational stored. Using another type of database is also 
an option (document based). However, for analysis tasks we are going to perform, 
this is not an ideal solution. These document databases are built for 'single' 
requests (e.g. to support an API with predefined requests), and not for 
analysis workloads (e.g. aggregating).
\end{description}
% ==============================================================================
The decision we made to load this dataset, is to load all subelements in a 
separate table (option 1). The main reason is the possibility for integrating 
this set with other sources. Given the example in Listing~\ref{lst:DblpExample}, 
this results in multiple tables: Article, which is the main object, Author, 
where all authors can be put and separate tables for title, journal, volume, 
month, year and url.
% By extracting these elements in a separate table we can normalise the data.
% We can even go further and normalise everything, which makes it easy to automate the load of data.

% This code part will then 
% \begin{description}
%     \item[All data] By using this approach, we load all the data. We do not need to specify which data we need at forehand, which is a tedious task with a huge XML file (the file is 3,4 GB).
%     \item[Easy to automate] We can automatically see all the attributes in the article element and create tables for the fields.
% \end{description}
% Drawbacks of this approach are:
% \begin{description}
%     \item[A lot of tables] For every element we need to create a table. This can decrease the overview of available data.
%     \item[Storage] Loading all the data comes with a cost in storage.
%     \item[Unnecessary data] All the data means also data we do not need.
%     \item[Relationships] Because the data is being splitted up accross mutliple tables, we need to keep the referential integrity. We need to add identification to all tables to know which article (or other parent object) is belongs to.
% \end{description}



% \begin{itemize}
%     \item Bibliografische records zijn gedefinieerd door een element met subelementen. Deze subelementen bevatten waardes, en geen andere elementen.
    
%     \item 
%     \begin{itemize}
%         \item zie author element in voorbeeld
%         \item moeten manier vinden om hier mee om te gaan
%         \item oplossing is id generereren voor elk 'parent' object.
%         \item dit zijn objecten die direct onder de tag <dblp> vallen.
%     \end{itemize}
   
    
    
%     \item deze objecten aparte tabellen opslaan
%     \item waarbij tabel naam de naam van het attribuut is (bijv. article)
%      \item ander probleem geen zuivere xml: 

% \end{itemize}





% \section{Overcoming the difference}
% \begin{itemize}
%     \item Tielemans vooral gebasseerd op beschikbare data (DBLP)
%     \item Machtsverhoudingen komen niet terug in de publieke datasets
% \end{itemize}
% \subsection{Justification}
% \begin{itemize}
%     \item Overzicht real-life cases (probleem verder beschrijven)
%     \begin{itemize}
%         \item Voorbeeld Vasilakos, citaten uit 2 journals waar hij zelf in zit
%         \item Publicatie lag
%     \end{itemize}
% \end{itemize}

% \subsection{Process and choices}




\section{Conclusion}
% Antwoord op welke data is nodig en hoe komen we eraan
In his chapter we defined the model which we need to fill to be able to do 
analysis on the publication data. Also, we explain how we can fill this model
with publicly available data source and what misses. The most important 
conclusion probably is that power relations in the scientific publication 
process are not present in publicly available datasets.

In the next chapters we will work out three case studies and dive deeper in 
the missing data and how we can acquire this.

% ==============================================================================
\chapter{Case study 1: Program Committee Members}
\label{chp:case1}
% ==============================================================================
This chapter focuses on the Program Committee Members as group. We will start 
with a motivation why this group is interesting. After this motivation we will
investigate how we can put the dataset together to perform analysis upon. 
We continue with performing some analysis which give (in combination with the 
other case studies) answer to the main question of this study. In the last 
section we will validate the results of the analysis.

\section{Motivation}
The most important motivation to investigate this group, is that this group is 
in a position with great power. In the end, these people decides which articles
(and which authors) get published. Implicit, they decide whose career is going 
to thrive.
This power comes clear in an experiment conducted on an Artificial Intelligence 
conference; the NIPS experiment.


% \begin{itemize}
%     \item PC Members are in a position with power.
%     \begin{itemize}
%         \item Power of this group indicated with NIPS experiment.
%         \item Decision who gets published, implicit who succeeds.
%     \end{itemize}
%     \item Possible to exploit this power.
%     \item PC Members with high impact power are on high impact conferences.
%     \item Thus, we are focussing on top conferences.
%     \item The goal with this case study is to see if we can identity outliers 
%     within a group of PC members.
% \end{itemize}
% \outline{
% \begin{itemize}
%     \item dat je ze krijgt niet interessant
%     \item wel waarom, en hoe goed
% \end{itemize}
% }

\paragraph{The NIPS experiment}
For scientists getting a paper in a conference is important for their career. 
Unfortunately, the NIPS experiments makes clear that reviewing papers for a 
conference is not a deterministic
process\footnote{\url{http://blog.mrtz.org/2014/12/15/the-nips-experiment.html}}.
If a paper gets approved depends on the composition of the 
responsible committee. This means that the committee judging papers for 
conferences has the power to determine whose paper is being accepted and 
therefore whose career is going to thrive.

\paragraph{}
PC Members are in a position where they can exploit this power. Members with
high impact, are members on high impact conferences. This reason, we focus on
top conferences. The goal with this case study is to see if we can identity outliers 
within a group of PC members.
An interesting case related to this group is described in 
Section~\ref{interesting_case:work_member_editorial_board_cited}.

% ------------------------------------------------------------------------------
\subsection{Interesting case: Work member of editorial board is being cited}
\label{interesting_case:work_member_editorial_board_cited}
% The editorial board can also be the program committee in case of incollections.
When an author becomes a specialist of a certain subject, it is not uncommon 
that this person becomes member of the editorial board or program chair of a 
venue that handles this specific subject.
New publications regarding this subject will likely be published in this venue. 
It is not strange that this publication cites work of the person-of-interest, if 
he is indeed a specialist.
In Figure~\ref{fig:cweb} we present an instance model of this situation. In this 
model, pers1 represents the person-of-interest. We notice the relation of this 
person to the venue (v1) as an editor and to previous work (p2) as an author. A 
new publication (p1) has a citation to the previous work of the 
person-of-interest (c1).

\begin{figure}[H]
\centering
\includegraphics[width=9cm]{images/cite_work_editorial_board.drawio.png}
\caption{Instance model of citation of work of member from editorial board}
\label{fig:cweb}
\end{figure}

% ------------------------------------------------------------------------------
\paragraph{Attack} When this situation is being enforced (malicious intent), it 
becomes attack (e.g. the editor says: "cite me, or your work will not be 
published"). The person of interest benefits 
from this situation; he will be cited more, which impacts his metrics (e.g. 
H-Index). The question is now how this enforcement impacts the 
publication model so we are able to detect this. 

% ------------------------------------------------------------------------------
\paragraph{Model impact}
The impact this attack has on the publication model is on the objects Venue, 
Person (with the role editor) and the relationship between. This relationship is 
the fact that a person works for a venue. In this relationship, citations occur; 
while a person works for a venue his work is being cited. In case of an attack; 
we see more citations here than normal. The metric we need from this 
relationship is the number of citations: \textit{number\_of\_cites}. But we need 
to take the time into account; if a person works for a venue for a long time (
multiple incollection or inproceedings are published), it is logic the number of 
cites while being a member is high. Because of this; we also need the number 
of incollection and inproceedings: \textit{number\_of\_issues}.
These metrics can be used for comparison by calculating the 'normal' and the 
deviation of all editors. This normal can be calculated over the venue the 
editor is working for, or all venues. This comparison makes it possible to 
identify interesting editors in comparison with his peers.

\begin{figure}[H]
\centering
\includegraphics[width=9cm]{images/cite_work_editorial_boardmodel_impact.drawio.png}
\caption{Metrics needed for detection on publication model}
\label{fig:cwebimpact}
\end{figure}

% \begin{itemize}
%     \item Het roept vraagtekens op als gepubliceerd werk referenties bevat van iemand in de editorial board of program committee.
%     \item Dit is niet per se slecht; maar lichtelijk vreemd.
%     \item Als de persoon in de board ook in andere publicaties geciteerd wordt op het moment dat deze persoon in de board zit, wordt de twijfel of dit nog ethisch is groter.
%     \item \todo{hoog aantal gaat vraagtekens opleveren}
%     \item members onderling vergelijken
%     \item kan verschillen per venue
% \end{itemize}

% ------------------------------------------------------------------------------
\paragraph{Beneign alternatives}
The described impact on the publication model can also have other beneign reason 
to occur. The detection strategy is not foolproof. We need to take into account 
that a person is actually that good, it is logic he is being cited far more that 
its peers.



% ==============================================================================
\section{Data acquisition}
In this section we will elaborate which data we need and how we get that data.

% ==============================================================================
\subsection{Top conferences}
The focus of this case study is people that came to a position with influence.
People with influence can be found at top conferences and journals.
Because of this, the primary selection criteria are top conferences and journals.
Considering the dataflow in Figure~\ref{fig:dataflow_jm2017_2}, this is the 
sigma-sign.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{images/data_to_publication_metrics_jm2017.png}
    \caption{Data flow (\cite{JM2017})}
    \label{fig:dataflow_jm2017_2}
\end{figure}

For our study, which focuses on computer science, we get the top conferences 
and journals from The Computing Research and Education Association of 
Australasia (core)\footnote{\url{https://www.core.edu.au/}}. Based on their 
ranking, we selected the conferences and journals for our research with a 
ranking of B or higher.

The CORE website provides an export functionality for this data. However, for 
integration purposes, this is not sufficient; the export misses the link to 
the DBLP conference, which is available on their website. For that reason, we 
built a webscraper which is able to get the necessary information for our case 
study (tool: core\_scraper) instead of using the download functionality.

The dataset misses 17 DBLP links which we added by hand.

% ==============================================================================
\subsection{Proceedings}
With the selection criteria in place, we can get the proceedings of the 
conferences from DBLP. DBLP provides the option to download the complete data 
dump from their website (described in Chapter~\ref{chp:data}). 
Unfortunately, conferences are not part of this dump. But, with the API of DBLP
we are able to get the proceedings based on the searchterm from the url of core.
So we built a tool to load this data from the API in the database (tool: dblp\_api).

In Figure~\ref{fig:data_integration_core_dblp_api} we see how this selection 
influence what is being gathered from dblp. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{images/data_integration/core_dblp_api.drawio.png}
    \caption{Gathering proceedings with selection criteria. The parallelogram are 
    (intermediate) datasets and the rectangles are process steps.}
    \label{fig:data_integration_core_dblp_api}
\end{figure}

Looking at the publishers of the proceedings we see the following publisher with 
more than a 1000 proceedings:
\begin{center}
    \begin{tabular}{ |c|c|c|c| }
        \hline
        Publisher & \# of proceedings \\
        \hline
        Springer & 4751 \\
        ACM & 4115 \\
        IEEE Computer Society & 2422 \\
        IEEE & 1210 \\
        \hline
    \end{tabular}
\end{center}
For this case study we want to use the biggest source, which is Springer. 
Springer publishes their proceedings under the Lecture 
Notes of Computer Science (LNCS). The resulting set contains 4467 unique 
proceedings (the DBLP API search function sometimes returns double results).

% ==============================================================================
\subsection{Eye on the ball; where are those PC Members?}
On the site of a LNCS proceeding, a document is available for download which 
includes the members involved in that specific conference; also the PC members. This 
document is called the Front Matter.

The data acquired from the DBLP api contains the doi link to 
the proceeding on LNCS (for 1 document this misses, so we added it manually). 
With this anchor we can start acquiring the necessary data, including the 
download link to the front matter document.

% ==============================================================================
\subsection{Proceeding information, articles and authors}
\label{subsec:springer_website}
To get the data from LNCS we built a webscraper (lncs, scraper). As input it 
takes the doi links from the dblp api and the output is a dataset with books 
(proceedings), chapters (article), editors and authors and their affiliation. 
See Figure~\ref{fig:lncs_scraper_datamodel} for a graphic representation of the 
entities and their relations.

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{images/lncs_scraper_datamodel.drawio.png}
    \caption{Datamodel of the output of the LNCS scraper}
    \label{fig:lncs_scraper_datamodel}
\end{figure}

The reason we also extracted the chapters is get the relation between a book and
the articles it contains. To keep our main goal clear: we need to get the people
involved, so while we are scraping the articles, we can scrape the authors just 
as well.

The most important entities we gather from LNCS are the books (proceedings), 
chapter (articles) and authors. The editors and their affiliation are not 
important for this case study. 

All chapters we scraped contains a DOI as attribute. 
By scraping the books we also get the link to the front matter pdf which can be 
used to download the front matter.

% ==============================================================================
\subsection{PC Members}
With a simple tool we downloaded all the front matter from the urls acquired 
from the previous step. An important step is to convert this collection of PDF 
files into information. Because of the effort and size of this step in the 
process, we dedicated a separate Chapter~\ref{chp:front_matter_parsing} on this 
step.

In Figure~\ref{fig:lncs_scraper_datamodel} we show the flow from the proceedings
to the PC Members.

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{images/data_integration/lncs.drawio.png}
    \caption{Process LNCS data}
    \label{fig:lncs_scraper_datamodel}
\end{figure}



% ==============================================================================
\section{Data integration}
In this section we explain how we integrate the data we collected from the
previous section. Not all sources need to be integrated:
\begin{description}
    \item[CORE] is used to filter the conferences on score. This source
    does not contain data that has to be integrated.
    \item[DBLP API] is used to get the proceedings of the conferences. Although 
    integration of this API source is not needed, we will integrate with 
    the bulk data from DBLP.
\end{description}



\subsection{Dataset definition}
For this case study we want two datasets.

% ==============================================================================
\subsubsection{Dataset: Published articles}
\label{subsubsec:dataset_published_articles}
This dataset contains information regarding publications members have published 
in their own proceeding. For this dataset we use two sources:
\begin{description}
    \item[Members] Our focus are the member from the proceedings, so the design of the dataset
    should start here. The pragmatic choice we made by using this set, is to only
    take validated data from the front matter documents; see 
    Section~\ref{sec:front_matter_validation}.
    \item[Authors publications] From these members we need to get the articles they wrote. We can use two sources
    for this information: DBLP and the data acquired from the Springer website.
    Because the memberset contains the proceeding the member belongs to, and the
    dataset from Springer also contains this proceeding where the article belongs to, 
    we choose to use this dataset.
\end{description}

In Figure~\ref{fig:members_integration_publish} the formalising of the dataset
is shown.

\begin{figure}[H]
    \centering
    \includegraphics[width=11cm]{images/data_integration/members/members_integration_publish.png}
    \caption{Formalising the publishing dataset}
    \label{fig:members_integration_publish}
\end{figure}

The Springer website part (left) is also described in 
Subsection~\ref{subsec:springer_website}. The interested entities from this set
are book (the proceeding) and author. To define the relationship which authors 
publishes in which proceeding, we use the chapter entity.

The front matter part (right) is the result after applying a filter to use 
approved data. 

The relationship between the two sources are base on `dblp key' and `name'. 
The dblp key is the identification from dblp for a certain proceeding. Because we 
use dblp as source fro gathering the proceedings from the Springer website we 
have the key given by dblp. Second, because the Springer website data is used to 
download the front matter documents, we can pass this dblp key to the members.
By taking this approach, we make the DBLP key the cross source key for 
identifying a proceeding.
The relationship on the `name' is to match the name of the member with the name
of the author. We use an optimistic approach here; just a join on the raw name.
If a name can not be matched, we accept this. As a recap; our purpose is to prove
that a group base approach can help identifying egregious people, we are not 
focusing on creating a foolproof end-to-end solution.
% ==============================================================================

\subsubsection{Dataset: Citation of members}

The sources we integrate for this dataset are:
\begin{itemize}
    \item DBLP (Referred articles, authors)
    \item OpenCitation (Citations)
    \item LNCS website (Articles, authors)
    \item Front Matter (PC Members)
\end{itemize}

In Figure~\ref{fig:members_integration_citation} we show the sources and their 
relationships to create this dataset.
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{images/data_integration/members/members_integration_citation.png}
    \caption{Formalising the citing dataset}
    \label{fig:members_integration_citation}
\end{figure}

\begin{description}
    \item[Members] Also in this dataset, the focus is on the members.
    \item[Articles] The Front Matter and Springer website are joined together 
    just as with the previous dataset, on dblp key. From this proceeded we
    can get the chapters (articles) involved.
    \item[Citations] Because we want to know which authors are being cited in 
    a proceeding, we need the citations. 
    For citations we described two sources in Chapter~\ref{chp:data} which may be 
    sufficient. The first is Aminer, which is a dataset that has the articles, 
    references and authors. The second is OpenCitations, which only contains DOI's. 
    As describer earlier, Opencitation is doi based and we have all the doi's from 
    the chapters from the Springer website. Because of this, we choose to use 
    OpenCitation as a source for references.
    The dataset from OpenCitation is a download, but very large. For this reason
    we choose to filter the data at loading, so we only load the references from
    articles in LNCS.
    \item[Referred articles] The referred articles we get from the DBLP dump.
    We could have used Aminer for this, but because the way DBLP handles 
    the names of the author, we choose DBLP.
    \item[Cited authors] in the DBLP set, we can get the authors of the referred
    articles. With this cited authors we can go back to the members.
\end{description}
% ==============================================================================
For laying the relationships, we use an optimistic approach:
\begin{itemize}
    \item We link the member name to the author name in DBLP without additional 
    logic. However, because we use DBLP we match against possibly multiple names
    of the author.
    \item Because we use OpenCitation, we only refer to articles in DBLP which 
    contain a DOI. Another consequence is that we rely on the completeness of
    OpenCitation, but this is an issue with every source we use.
\end{itemize}

\subsubsection{Added value of these datasets}
By having these datasets we can calculate citation and publication metrics for 
members and 
compare them with their peers, which may be in the same proceeding or across 
proceedings, depending on the scope of the peers.


% \subsection{People}
% The author dataset contains all authors for every article. This means, if an 
% author wrote multiple articles, it is multiple times present in this set. For
% our use case, this is not sufficient; we need identify these people as the same 
% person, otherwise collecting metrics for a person is impossible.

% The set we achieved from LNCS conains name, orcid and email. An analysis of this
% set made clear that we got the same name with multiple orcid, or one orcid that 
% refers to more than one person. Concluding: a name does not make a person 
% unique.

% Luckely, DBLP acknowledges this problem. Although they do not yet have all 
% authors unique, they are working on this problem and provide a way to get the 
% unique persons already identified.

% DBLP has the relation between author and article, so to get the `correct' author
% of an article, we need to match the articles in DBLP with the chapters in LNCS.


% \subsection{Integration of persons}

% First challenge is to make the authors unique. This comes with the
% second challenge of data quality.

% For example, from springer we scrape get the chapters (inproceedings) and their 
% authors from their site. This authors sometimes comes with an orcid, which 
% should be unique. However, we found situations where an author gets an orcid of
% someone else. This is a data quality issue.
% \todo{voorbeeld toevoegen}
% Luckely, this does not occur much, and for the few it occurs we can easily 
% create a manual file to override this. Of course with caution; if two orcid 
% have the same name, we can not override this; some authors have the same name.
% On the other hand, if one orcid refers to a totally different name, we can.

% Seconds challenge is that a name does not make an author unique. The same name
% refers to other authors. Luckely, DBLP addresses this problem by suffixing the
% name with a four digit code~\footnote{\url{https://dblp.org/faq/1474704.html}}.
% DBLP is making progress here, but this is not entirely done yet. But even if
% they mention to unique identity authors, the match to authors in other 
% datasources (like Aminer and Springer) is not straightforward. In that case we 
% need to ignore the authors from these sources and make the match by going 
% through the paper (or chapter, as is it is called within LNCS Springer).

% Third challenge is the name. For most authors these sources do not contain an
% orcid or an email address, which means we need to match the authors on name.
% But, in some sources the names are fully written (with or without the 
% middlename), sometimes the middlename only has a initial while the firstname is 
% fully written, and sometimes we only get the initials and lastname.
% DBLP covers this by naming all aliases of a single person.

% Still we do have a problem, because in the end we need to match these authors
% with members we get from the frontmatters of lncs. Within this frontmatters, 
% there is no orcid, email, or a four digit number to match to dblp records; these
% documents or not written with the intention to identify persons.


% ==============================================================================
\section{Analysis}
\outline{
\begin{itemize}
    \item Wat betekent de data?
\end{itemize}
}
With the described datasets we can calculate for every member what the deviation
is for citationcounts and publicationcounts compared with his peers. 
In Figure~\ref{fig:analysis_members} we plot the ratio of the deviation for 
publishing and citation. Every point represents an member.
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{images/analysis_members.png}
    \caption{Analysis of members a proceeding}
    \label{fig:analysis_members}
\end{figure}

As deviation we used the Median Average Deviation score.

% ==============================================================================
\section{Validation}
\outline{
\begin{itemize}
    \item Werkt je methode
    \item Aan het eind van ieder hoofdstuk of een eigen hoofdstuk
\end{itemize}
}


% ==============================================================================
\chapter{Front matter parsing}
\label{chp:front_matter_parsing}
% ==============================================================================
\begin{itemize}
    \item There is more to the publication process than the data
        in A-miner / DBLP can tell you.
    \item Examples: PC memberships, editorships, reviewers (who reviewed which paper),
        how fast was the review process, etc.
    \item Some of this data is publicly available, but not integrated into existing
        publication data sets.
    \item For example: Elsevier gives timelines of paper revisions for journal articles;
        in their LNCS series, Springer typically lists the PC members in the front matter.
    \item This data may be useful for identifying outliers.
    \item In this chapter, we investigate how to extract both Elsevier and LNCS data and
        how to integrate them with existing data sources.
    \item
    \item People involved in the proceedings are mentioned in the Front Matter.
    \item Front Matter can be downloaded as PDF.
    \item PDF are considered unstructured data.
    \item PDF's are meant to read by humans, not computers. However, currently 
        we have 4370 Front Matters, which is an unbearable task to copy and 
        paste the data manual from these PDF's.
    \item To be able to get the data inside these PDF's, we need to convert the 
        PDF's to a structured or semi-structured dataset.
\end{itemize}

\section{Reading a PDF}
\begin{itemize}
    \item Multiple options to make the pdf computer-readable:
    \begin{description}
        \item[Convert to HTML] There are multiple tools available to convert PDF
            to html format. Further processing can be applied by using HTML
            libraries. Unfortunately, this HTML pages are complex to interpret
            because a tag can contain just one letter. We need to glue these 
            tags together, with taking the location of the tag in consideration
            (how much space is between two letters, is that a space, is that a 
            new column, or an indent in an existing column etc.). We did an 
            experiment to process PDF's this way, but the resulting solution to 
            process one PDF was not able to process another one. The effort to 
            make this a generic solution which can easily be extended where 
            necessary was very high in comparison with the result we achieved.

        \item[Convert to plain text] Almost all libraries to read a pdf file 
            support the option to read the file as plain text. Drawback is that
            the lay-out is gone. This lay-out is necessary to get some structure
            from the document.
        
        \item[Convert to image and extract text] This was we can use machine 
            learning liberaries to get the text from an image. We tries this 
            with Tesseract, but too much information was lost to get the 
            structure of the document.

        \item[Use low level library] In this experiment we tried of we were able
            to process the PDF with a more low level library. This give us more
            control of the process. First small experiments with PDFBox were 
            succesfull.
    \end{description}
\end{itemize}

\section{A prototype extractor of raw data}
\todo{We chose option 4 because \$REASONS.}
\begin{itemize}
    \item First we need to get the raw data from the PDF's.
    \item PDFBox has an parser that gives the PDF back as text. As mentioned
        before, for our case this is not enough. The problem is within this text
        all spaces between words are converted to one space. However, we need to
        be able to identify columns, so we need to keep all spaces.
    \item Luckely, we were not the only one with this problem. On 
        Github\footnote{\url{https://github.com/JonathanLink/PDFLayoutTextStripper}} 
        there is an open source additional parser available for PDFBox that 
        keeps the layout as is.
    \item However, this extension still did not have all properties of a text we
        need to get the structure of a file. Luckely, because this extension 
        issue open source, we can add this information to the text objects we 
        read from the PDF.
    \item Besides the text with all the spaces in between we get by using this 
        extension, we also added:
    \begin{itemize}
        \item FontSize
        \item FontSizeInPt
        \item XScale
        \item IsBold
    \end{itemize}
    \item The result is now that we have the pdf file as textobjects with 
        extra properties.
\end{itemize}

\subsection{Building a document tree}
\label{sec:lncs_parser_doc_tree}
\begin{itemize}
    \item By using the properties textsize and bold we are able to build an 
        hierarchical structure of the document. 
    \item We need this, because in most
        Front Matters there is an organization header, with subheaders 
        containing the role of the people.
    \item As datastructure to work with the data we use a tree.
    \item The objects we store in this tree are Sections. These contain the 
        textlines and the header as title of the section.
\end{itemize}

\begin{lstlisting}[caption={Representation of the document},captionpos=b]
{"section":{"title":"DocumentRoot","# lines":1}}
- {"section":{"title":"Lecture Notes in Computer Science 7727","# lines":7}}
- - {"section":{"title":"Editorial Board","# lines":32}}
- {"section":{"title":"Kyoung Mu Lee Y asuyuki Matsushita James M. Rehg Zhanyi Hu (Eds.)","# lines":1}}
- {"section":{"title":"Computer V ision - A CCV 2012","# lines":3}}
- - {"section":{"title":"11th Asian Conference on Computer Vision Daejeon, Korea, November 5-9, 2012 Revised Selected Papers, Part IV","# lines":22}}
- - {"section":{"title":"1 3","# lines":19}}
- - - {"section":{"title":"Preface","# lines":77}}
- - - {"section":{"title":"Organization","# lines":8}}
- - - - {"section":{"title":"Steering Committee","# lines":7}}
- - - - {"section":{"title":"General Chairs","# lines":7}}
- - - - {"section":{"title":"Program Chairs","# lines":7}}
- - - - {"section":{"title":"Workshop Chairs","# lines":8}}
\end{lstlisting}


\subsection{Focus on organisation}
\begin{itemize}
    \item Having this tree, we can easily get the section that contains the 
        organisation (because this section is called, you have never guessed it, 
        'organisation').
    \item Because we have a tree data structure, we can ask this organisation 
        section to loop trough all sections underneath.
\end{itemize}

\subsection{Processing a section}
A section is a textpart of the document. In 
Figure~\ref{fig:front_matter_section} an example is shown.
\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{images/front_matter/section.png}
    \caption{Section in the Front Matter}
    \label{fig:front_matter_section}
\end{figure}
\begin{itemize}
    \item All text in a section, has some kind of table 
        struture. In the example, the role of the member is in the title.
    \item For that reason we use a grid a data structure to put the data from 
        the raw lines in to.
    \item This grid structure gives us some possibilities, e.g. to get 
        statistics of a column.
    \item The challende in this part is to merge lines into one line.
    \item This process is done in a section to grid converter. To separate this
        process from the process of building a tree, improves testability.
\end{itemize}
The resulting visual representation is shown in 
Figure~\ref{fig:front_matter_section_grid}.
\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{images/front_matter/section_table.png}
    \caption{Section in the Front Matter}
    \label{fig:front_matter_section_grid}
\end{figure}

\subsection{Getting information from a grid}
\begin{itemize}
    \item We calculate some statistics of a grid which we need to choose a
        parser further down the processing.
    \item The statistics we get:
    \begin{description}
        \item[Number of textparts] The total number of cells that are filled in
            the grid.
        \item[Number of columns] The number of columns that the grid contains.
            Most sections contains two columns, but this is certainly not the 
            case for all sections.
        \item[Affiliation ratio] The ratio of textparts that contain keywords 
            for an affiliation (e.g. "universi"). This is calculated for every 
            column and for odd, even and all rows.
        \item[Comma ratio] Same as affiliation ratio, but for comma's. Can also
            be used to identify affiliations. However, names also contains 
            comma's if the format is lastname, firstname.
    \end{description}
    \item All this information is stored in a SectionInfo object which is added
        to the section.
\end{itemize}
In Listing~\ref{lst:front_matter_grid_info} we show some statistics for 
identifying affiliations.
\begin{lstlisting}[caption={Some derived information from the grid},captionpos=b,label={lst:front_matter_grid_info}]
numberOfColumns:	2
affiliationRatios:
	{"columnNumber":0,"rows":"ALL","ratio":0.0}
	{"columnNumber":0,"rows":"ODD","ratio":0.0}
	{"columnNumber":0,"rows":"EVEN","ratio":0.0}
	{"columnNumber":1,"rows":"ALL","ratio":0.6666666666666666}
	{"columnNumber":1,"rows":"ODD","ratio":0.5}
	{"columnNumber":1,"rows":"EVEN","ratio":1.0}
\end{lstlisting}

\subsection{Choosing a parser}
\begin{itemize}
    \item With the information the program can choose the appropriate parser to
        interpret the data correctly and convert the grid into members.
\end{itemize}
In this case, because the ration of affiliation keywords in the second column is 
above a certain threshold and the section consists of two columns, the software 
chooses a Two\_Name\_Affiliation parser which means two columns, first column is a 
name, and the second column is the affiliation.

\section{Data storage}
\begin{itemize}
    \item During this process we store data in a database and in logfiles.
\end{itemize}


\subsection{Database}
In the database we store three entities: file, section and member. All tables 
contain at least the following attributes:
\begin{description}
    \item[id] Auto generated number.
    \item[run\_id] The identification of the program run (epoch).
\end{description}
In Figure~\ref{fig:lncs_pdf_database} the relationship between the tables are
shown.
\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{images/lncs_pdf_parser_tool.drawio.png}
    \caption{Relationship between tables}
    \label{fig:lncs_pdf_database}
\end{figure}

\subsubsection{File}
This table contains information of the file. It can be use to link to previous 
gathered data. The following attributes are in this table:
\begin{description}
    \item[filename] The filename of the pdf to refer to other tables in the 
        process.
    \item[status] If the process of the file succeeded or failed.
    \item[message] If the process failed, how come. 
\end{description}

\subsubsection{section}
This table contains information of the section. The following attributes are in 
this table:
\begin{description}
    \item[title] The title of the section (e.g. Program chair).
    \item[num\_parts] The number of textparts that are detected in this section.
    \item[num\_section\_lines] The total number of lines in a section.
    \item[num\_section\_lines\_non\_empty] The number of lines in a section that
        are not empty.
    \item[num\_merges\_lines] The number of lines that are merged in to the 
        previous line.
    \item[num\_grid\_rows] The number of rows the grid has.
    \item[num\_grid\_columns] The nubmber of columns the grid has.
    \item[all\_values\_contain\_commas] Indicator if all values contain commas.
    \item[comma\_ratios] A json structure with statistics of comma's in the text.
    \item[affiliation\_ratios] A json structure with statistics of affiliation 
        keywords in the text.
    \item[parser] The parse the program choose to process the section.
\end{description}

\subsubsection{Member}
This table contains eventually the resulting data we are interested in.
\begin{description}
    \item[role] The role of the member. This is in most cases the same as the 
        the title of the section (but, not always). 
    \item[name] The name as we get from the data, whatever format it is. If we
        get a separate firstname and lastname from the section, the name is 
        built based on those values. However, we prefer separate firstname and
        lastname, because this gives us more options when we need to integrate
        this data with data from other sources.
    \item[firstname] If we are able to get separate firstname and lastname of
        the member, we will store this here. We do tend to interpret names which 
        contain a comma as <lastname>, <firstname>.
    \item[lastname] Same as before, but then the lastname :).
    \item[affiliation] The affiliation of the member. This data is not always
        available, but we do not tend to use it.
\end{description}

\subsection{Iterative Logging-based development process}
For every file that the program process, it keeps a separate logfile. This gives
us the ability to investigate why the program was unable to process some files 
or sections. Missing parser for example can be detected using this mechanism in
combination with the data in the section table in the database.

For every file the logfile shows the document tree as described in 
Section~\ref{sec:lncs_parser_doc_tree}. 

If sections are found, the logfile shows for every section:
\begin{description}
    \item[Textlines] All the lines that are in this section.
    \item[Grid] The derived grid from the textlines.
    \item[Members] The members that are extracted from the section.
\end{description}
This may seem a lot, but having this information gives us the possibility to 
investigate in detail why data can not be interpreted and easily build new 
functionality (e.g. new parsers) and add unit tests for this functionality.

\section{Validation}
\label{sec:front_matter_validation}
Currently 4370 Front Matters are available. How are we able to tell something 
about the quality of the program without manually go through the results?

For every section we have some metrics. With these metrics, depending on the
parser we can see if the number of extracted members is correct, or close to
correct.

For example, The parser Two\_Role\_NameAff contains two columns: first with a name
and second with the affiliation. We can measure if the number of resulting 
members is equal to the non-empty lines in the section minus the merged lines.



\subsubsection{Current state}


\begin{figure}[H]
    \centering
    \includegraphics[width=17cm]{images/lncs_front_matter_result.drawio.png}
    \caption{Current results}
    \label{fig:lncs_pdf_database}
\end{figure}


% ==============================================================================
\chapter{Case study 2: Journal editors}
\label{chp:case2}
% ==============================================================================
In this chapter we work out a case study which focuses on journal editors of 
ACM (Association for Computing Machinery)\footnote{\url{https://dl.acm.org/}}; 
a publisher of journals in the area of computer science.
With journal editors we mean the Editor-in-Chief and the associate editors.
As with the previous case study, we will start with a motivation and example of
an attack, proceed with the data acquisition and integration and conclude with
an analysis of the data and validation of the analysis.

% ============================================================================
\section{Motivation}
This case study focuses on journal editors, which are in position with great 
power. As previously shown in Figure~\ref{fig:c2013}, both editor roles are in 
a position power to make or break a publication and the career of an author.
Let us start with an interesting case that may be of an attack that can be 
executed while on this position.

An interesting case for this group is described in 
Section~\ref{interesting_case:member_editorial_board_is_coauthor}.

\subsection{Interesting case: Member of the editorial board is (co-)author}
\label{interesting_case:member_editorial_board_is_coauthor}
In this case, publications in a venue are co-authored by a member of the 
editorial board, which means he is publishing his own work. A questionable 
case in this situation is
Griffiths\footnote{\url{http://deevybee.blogspot.com/2020/07/percent-by-most-prolific-author-score.html}}.
Except for editorial boards, this can also happen for conferences (the published 
dataset in Section~\ref{subsubsec:dataset_published_articles} can make this 
clear for PC Members).
In Figure~\ref{fig:eia} we present an instance model of this situation.
\begin{figure}[H]
\centering
\includegraphics[width=3cm]{images/editor_is_author.drawio.png}
\caption{Instance model of the situation where the editor is (co-)author of 
the publication}
\label{fig:eia}
\end{figure}


\paragraph{Attack}
We can imagine a situation where the author is being forced to mention the 
person-of-interest as co-author. Which is abuse of the power of the editor.

\paragraph{Model impact}
The impact on the publication model is between the person which is editor for a 
venue, and the venue itself. 
The number of publications he has in that venue while he works for that venue 
will increase. For detection we can measure how much he publish in the Venue 
when he work for that specific venue.
One metric we need here is \textit{number\_of\_publications}. Also with the 
case~\ref{interesting_case:work_member_editorial_board_cited}, we need the 
number of issues.

% ------------------------------------------------------------------------------
\paragraph{Beneign alternatives}
Besides possible ethical questions that can be raised, there is a perfectly good
reason for someone to publish in the venue he is editing. Say, a person is a 
specialist in his field and works at a venue which publishes about this 
specific subject. He has new work to publish but there is not another venue 
which captures this subject.


\subsection{Case study purpose}
\label{subsec:case2_purpose}
As with all case studies we try prove that applying a group based outlier 
detection is a feasible manner for fraud detection. In this case study the group
of focus is the editorial board.

The next question is how can be compare members of this group, what properties
makes them different and may be indicating an attack on the publication model.

As stated in the interesting case described in 
Section~\ref{interesting_case:member_editorial_board_is_coauthor}, a 
possible attack if being forced 
to add a member of the editorial board as co-author. One possible detection 
method is to see the ratio of unique authors the person-of-interest published
in the journal and outside the journal. If a member publishes inside the journal
with much more unique authors than outside the journal, this may indicate this 
member forces to make him co-author.

% ==============================================================================
\section{Data acquisition}
In this case study our focus is the editorial board. Editor-in-chiefs, 
associate-editors and sometimes a list of reviewers (generic, not who reviewed 
what paper) are most of the time available on the publisher website of the 
specific venue. But this has two issues:
% --------------------------------------------------------------------------
\begin{description}
    \item[Machine readability] These roles are not easy machine readable 
    through an interface. It is placed on the website for humans to read. 
    Although good technology exists to scrape this data, a minor change to 
    the website can break this method and is not a sustainable solution.
% --------------------------------------------------------------------------
    \item[Issue- and time awareness] Only the current editorial board is 
    shown on the publisher website. This may be sufficient for guests of the 
    site, but extending the datamodel with the editorial board with the 
    purpose of detecting fraud, this is not sufficient. In that case we need 
    to know who was editor for which issue. 
\end{description}
% --------------------------------------------------------------------------
Not everything is lost however; most of the time the previous editorial 
boards are available in so-called mastheads of a certain issue. Most of the 
time this can be downloaded as a PDF (as article of a journal). A formal 
investigation during the research preparation learned us that the format in 
which the editors are presented is not consistent (comparable with the 
front matter of the LNCS proceedings).

Theoretically, we can process the data like we did with the LNCS Front 
Matter. However, our intention is to prove that an outlier detection 
approach within the group of editorial board members is feasible. As we 
already gathered data from PDF's in Case study 1, we will not repeat this 
technical step (we already show that this is possible) and focus more on 
the functional added value of having editorial board members.

We choose ACM because ACM is a big publisher in the area of computer science
and they present the editorial members in a structured manner on their 
website for all journals. This makes acquiring the data in a automatic 
manner possible.

To gather the editorial board from ACM we first listed some journals where 
we want to get the editorial board from. From this list we scraped the 
editorial board. Figure~\ref{fig:acm_editorial_board} shows a screenshot 
from an editorial board page from ACM.

\begin{figure}[H]
\centering
\includegraphics[width=11cm]{images/acm_editorial_board.png}
\caption{Page with editorial board from ACM (\url{https://dl.acm.org/journal/csur/editorial-board})}.
\label{fig:acm_editorial_board}
\end{figure}

By building a tool for scraping these sites we can get the role, name, 
affiliation, country and journal name as attributes from this page.

% ==============================================================================
\section{Data integration}
As stated in Section~\ref{subsec:case2_purpose} we want to have the number of
unique co-author a member publishes inside and outside a journal. For this we
need the following entities:
\begin{itemize}
    \item Editors
    \item Articles written by these editors
    \item Co-authors of these articles
    \item The venue where these articles are published
\end{itemize}

We already acquired the editors from the ACM website. The other entities can be 
found in DBLP. 
However, the venue is not straightforward in the dump of DBLP; it is 
encapsulated in the key of an article, see Listing~\ref{lst:dblp_object_key} 
as example.

\lstset{language=XML}
\begin{lstlisting}[caption={Example DBLP key},label={lst:dblp_object_key}]
journals/tomccap/Wang21
\end{lstlisting}

In case of the example, `tomccap' is the abbreviation of the journal. However, 
the we get from ACM website is only the full name of an journal. In this case
`ACM Transactions on Multimedia Computing, Communications, and Applications'.
More interestly; this full title is also abreviated as `tomm'. Thus the relation
between a journalname and abbreviation is not 1:1.

% ------------------------------------------------------------------------------
\subsubsection{Journal abbreviation lookup}
We need a dataset to have this relation. Two sets were found:
\begin{description}
    \item[Pages at Clarivate] which are the maintainers of Web of science. 
    Unfortunately, the abbraviations used by DBLP were not found in this set.
    \item[University of British Columbia] also has a list. However, the 
    abbreviations they use are different. E.g. in case of the example, their
    abbreviation is `ACM Trans. Multimedia Comput. Commun. Appl.'. Luckely, DBLP
    also provide journalnames in this kind of format. Unfortunately, the 
    abbreviations from this university are not found in DBLP.
\end{description}
The list of journals we want to scrape from ACM is not that large: only 27. So
a manual built dataset is the way we tackle this issue. This is very achievable 
with help of the search functionality on the DBLP site. See 
Figure~\ref{fig:dblp_search_result} as example; the abbreviations used are shown 
directly.
\begin{figure}[H]
\centering
\includegraphics[width=11cm]{images/dblp_search_result.png}
\caption{DBLP search result (\url{https://dblp.org/search?q=ACM+Transactions+on+Multimedia+Computing\%2C+Communications\%2C+and+Applications}).}
\label{fig:dblp_search_result}
\end{figure}
% ------------------------------------------------------------------------------
\subsubsection{Integration model}
With the information from the previous section, we can now build the dataset to
perform our analysis upon. The dataset formulation is shown in 
Figure~\ref{fig:editors_integration_coauthor}. The explanation will follow 
after the image.
\begin{figure}[H]
    \centering
    \includegraphics[width=13cm]{images/data_integration/editors/editor_integration_coauthor.png}
    \caption{Formalising the publishing dataset}
    \label{fig:editors_integration_coauthor}
\end{figure}
As stated we start with the editors from the ACM website. By joining the name 
to the names in DBLP, we can get the unique DBLP name key (because of the 
issues with people covered in Section~\ref{sec:integrability_people}). With this 
key we can get all the articles written by the editor from the `article person'
table. By joining again with this table on the article key we can get the DBLP 
person keys for all co-authors.

To know if an article is published in the same journal the person-of-interest 
is member of, we need to identify if the journal from the ACM website is the 
same of the article published. In the previous section we elaborated on the 
issues involved here. By using a manual dataset as link table, we can identify
of the article is published in this particular journal. For this scenario we
extracted the journal abbreviation from the DBLP article key.

Additional information about the article can be achieved from the article table.

% ==============================================================================
\section{Analysis}
\outline{
\begin{itemize}
    \item Wat betekent de data?
    \item Alleen laatste jaar want; alleen actieve editorial board
\end{itemize}
}
Our analysis is bases on how many coauthors a member has only published once 
with in the journal, but did not publish outside the journal with. In 
Figure~\ref{fig:editors_integration_coauthor_analysis} boxplot is shown for
some journals of ACM for the year 2020. 
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{images/data_integration/editors/editors_coauthor_analysis.png}
    \caption{Outlier analysis of the use of coauthors}
    \label{fig:editors_integration_coauthor_analysis}
\end{figure}
Interesting to see is that for two journals outliers are identified (visible 
with a red circle). 

% ==============================================================================
\section{Validation}
\outline{
\begin{itemize}
    \item Werkt je methode
    \item Aan het eind van ieder hoofdstuk of een eigen hoofdstuk
\end{itemize}
}
% ==============================================================================
\chapter{Case study 3: Authors}
\label{chp:case3}
% ==============================================================================
\section{Motivation}
Section~\ref{interesting_case:author_reviews_own_work}

% ------------------------------------------------------------------------------
\subsection{Interesting case: Author reviews his own work}
\label{interesting_case:author_reviews_own_work}
This is off course a major attack on the integrity of the publication model. It 
should never be the case that an author becomes in a position where he can 
review his own work. Unfortunately, we know of such case with Moon. In 
Figure~\ref{fig:air} we present an instance model of this situation. We have 
one person (pers1) who have multiple roles in relation with one publication (p2).

\begin{figure}[H]
\centering
\includegraphics[width=3cm]{images/author_is_reviewer.drawio.png}
\caption{Instance model where author is also the reviewer}
\label{fig:air}
\end{figure}
% ------------------------------------------------------------------------------
From a set theory perspective we can describe this situation as: 
$\{h \in \Humans \mid \authors(p, h) \land \reviews(p, h)\}$.

In theory this situation should not be hard to found. However, besides 
availability of the necessary data, one who will execute such an attack, will 
most likely not give his own name as reviewer at submitting his paper (it is 
normal to identify possible reviewers at submission). The main question here 
is, how are we able to identify the author and reviewer as the same person.

We can also see this from a timeline perspective, which is how Moon is 
detected. In Figure~\ref{fig:timeline} we draw a timeline with the milestones of 
the publication process. Submitted is when the paper is received by the venue. 
Revised is the moment a paper is submitted again after review with changes. The 
asterisks means that this can occur multiple times, or none at all if the paper 
is good at first submission. Published is the last step and is the date the 
paper is published.
% ------------------------------------------------------------------------------

\begin{figure}[H]
\centering
\includegraphics[width=9cm]{images/timeline.drawio.png}
\caption{Milestones in timeline publication model with publication lag}
\label{fig:timeline}
\end{figure}

From this process we can measure the time between submission and acceptance 
(publication-lag) and use this measure as comparison with other authors en within 
the same venue. A low publication-lag compared with 'normal' can be used to 
identify interesting cases like an abnormal short time of review.
% ------------------------------------------------------------------------------
\paragraph{Beneign alternatives}
The possibility exists that a paper is that good, no revision is needed and the 
review did not take that much time. This also results in a low publication-lag.

% ------------------------------------------------------------------------------


\section{Data acquisition}

% ------------------------------------------------------------------------------
\subsection{publication lag}
% ------------------------------------------------------------------------------
While writing the proposal for this study, Scanff et al. published a preprint of 
their research about editorial bias~\cite{SNCMBL2021}. One part this research is 
the relationship between the publication-lag and prolific authors, which may be 
a member of the editorial board. This study makes clear that dates about the 
publication process are interesting features. DBLP and Aminer only provide the 
publication date. To enrich our initial dataset based on DBLP and Aminer with 
dates about the publication process, we need other sources. One of the journals 
that presents publication process dates of their articles is Elsevier. We 
conducted an experiment to verify if the data is obtainable.

As it turns out, when Elsevier presents an article on their website, a JSON 
document with details about the article is available in the HTML document. Among 
other details, this JSON contains a date section, see 
Listing~\ref{lst:ElsevierJsonDates}.

\begin{lstlisting}[caption={Dates in the JSON document},label={lst:ElsevierJsonDates}]
"dates": {
    "Available online": "25 January 2021",
    "Received": "16 July 2020",
    "Revised": [
        "14 December 2020"
    ],
    "Accepted": "4 January 2021",
    "Publication date": "25 January 2021"
}
\end{lstlisting}

By using web scraping and HTML interpretation we extracted the JSON documents 
with metadata of articles of 20 journals from Elsevier (Elsevier runs 2751 
journals). This resulted in a total of 7334 documents. We were able to calculate 
the average publication lag per journal and filter on the articles for which the 
publication lag was significant lower. A quick look at the editorial board of 
that specific issue showed that one of the authors was indeed part of the 
editorial board. Based on this small experiment, we conclude that it is 
possible to get the publication lag for most articles of Elsevier journals.


\begin{figure}[H]
\centering
\includegraphics[width=5cm]{ACM_Digital_Threats_Research_and_Practice.png}
\caption{Publication history of an article in ACM (\url{https://dl.acm.org/doi/10.1145/3442445})}
\label{fig:acm_dates}
\end{figure}
\begin{itemize}
    \item A paper goes through certain stages until it is published.
    \item Not all publishers expose the dates these stages are achieved. 
    \item Fortunately some do. For example ACM (\ref{fig:acm_dates}) and Springer. However, this is not for all publications available. So there are some missing data points.
    \item Fraud cases where this information can be helpful:
    \begin{itemize}
        \item Moon was detected because the time that his work was submitted and was approved by the reviewer (which was Moon himself) was remarkable short.
        \item ~\cite{SNCMBL2021}
    \end{itemize}
\end{itemize}

\section{Data integration}

% ==============================================================================
\section{Analysis}
\outline{
\begin{itemize}
    \item Wat betekent de data?
\end{itemize}
}
\begin{itemize}
    \item 1 authors with 2 publications that has a z-score > 2
    \item Not notorious
    \item 
    
\end{itemize}

% ==============================================================================
\section{Validation}
\outline{
\begin{itemize}
    \item Werkt je methode
    \item Aan het eind van ieder hoofdstuk of een eigen hoofdstuk
\end{itemize}
}
\begin{itemize}
    \item Onbekend of deze methode werkt
    \item Mogelijk te weinig data
    \item Exploratieve analyse leken wel een verband aan te tonen tussen de
    publication lag en de editorial board. With this case study it is not 
    possible to verify this statement. The editorial board data for the analysed
    journals are not acquired. 
    \item For possible futher study; this may be a good statement to 
    investigate.
    \item Unable to say something about the found oberservations; we do not know
    who is the reviewer. This data set is unavailable.
    \item Initiative: Open Peer Review (\url{https://www.fosteropenscience.eu/})
\end{itemize}

% ==============================================================================



% ==============================================================================
% \subsection{Roles in the editorial process}
% In the editorial process multiple roles are involved. These roles are executed
% by persons. We define all persons involved somehow in the editorial process by 
% \textit{H}. Publications are defined by the letter \textit{P}.
% % ------------------------------------------------------------------------------
% \paragraph{Author}
% \begin{itemize}
%     \item How evident, this role writes the text to be published.
%     The main question is, how is someone with this role able to influence the process in his favor.
%     One way is to suggest a reviewer and indicate non-preferred reviewers \cite{C2013}. This relates to the case of Moon.
%     \item Suggest Associate Editor
% \end{itemize}
% We define the collection authors of a paper $A(p) = \{h \in H \mid \authors(p, h)\}$

% % ------------------------------------------------------------------------------
% \paragraph{Editor-in-chief}
% Wie bepaalt wie de editor-in-chief is? -> macht
% \begin{itemize}
%     \item receives the manuscript $\receives(p, h)$
%     \item Performs initial check (relevance, suitability to undergo peer review)
%     \item Assigns Associate Editor based on Area of expertise and avoiding potential conflicts of interest between author and AE
%     \item end decision if paper is being published: Stel h = Editor in chief en p = publication dan: $\accepts(p, h)$ of $\rejects(p, h)$
%     \item An editor-in-chief has this role for a certain period (span multiple issues).
% \end{itemize}


% % ------------------------------------------------------------------------------
% \paragraph{Editorial assistance}
% \begin{itemize}
%     \item Checks similarity to other publication (iThenticate)
%     \item Verder geen invloed op process
% \end{itemize}

% % ------------------------------------------------------------------------------
% \paragraph{Reviewer}
% \begin{itemize}
%     \item Reviews a paper
% \end{itemize}
% We define the collection reviewers of a paper $R(p) = \{h \in H | reviews(p, h)\}$
% % ------------------------------------------------------------------------------
% \paragraph{Subeditor}
% \begin{itemize}
%     \item Niet echt relevant
%     \item Om tekst leesbaarder te maken
% \end{itemize}
% % ------------------------------------------------------------------------------
% \paragraph{Associate editor}
% \begin{itemize}
%     \item Identify and Assigns reviewers
%     \item administrative reject (desk reject): vaak al in eerder stadum eruit 
%         gehaald door Editor-in-Chief of Editorial assistance
%     \item recommendation to editor in chief
%     \item Following roles sometimes combined into the Associate Editor:
%     \item Managing editor
%     \begin{itemize}
%         \item Checks journal standards, word length, use of internal reporting 
%             standard
%         \item assigns editor
%         \item assigns reviewers
%     \end{itemize}
%     \item Editor
%     \begin{itemize}
%         \item Initial check
%     \end{itemize}
%     \item This role is also for a period of time
% \end{itemize}
% ------------------------------------------------------------------------------



\chapter{VANAF HIER ONZIN}

% ==============================================================================
\section{General process}
Bjorn and hedlund descirbed the publication process from a cost perspective 
\cite{BH2004}. We can use this to visualize the publication process from a time 
perspective. This view is input to determine if we are able to collect all 
necessary metrics.



% ==============================================================================
\section{Flow proceedings and conferences}


% ==============================================================================
\subsection{Group identification}
% ==============================================================================
The first step in detecting outliers is to identify groups of people who are in 
the same position. We identify the followig groups in his research because they
may be interesting.

\paragraph{Editors of proceedings}

\paragraph{Authors submitting at the same conference}

\paragraph{Author submitting at the same journal}

\paragraph{Authors that publish with each other}

\paragraph{H-Index}



% ==============================================================================
\chapter{Data integration}
% ==============================================================================

\section{References}
\todo{misschien naar acquisition}


\todo{iets over completeness}


\subsection{Hiding our identity}
\vraag{waar moet dit}

In this research we scrape a lot of data from websites.
During our research preparation we got blocked by Science Direct 
because we scraped the site with the same IP. To work around this problem of 
being caught, two options exists.

The first option is the use of a Virtual Private Network. Some free VPN 
providers exists and scraping sites using a VPN hides the IP for the website; 
the website only got the IP from the VPN server. The drawback is that the VPN
provider may get blocked.

The second option is using the TOR project as a proxy. TOR is a network of 
servers that hides the identity from the requester. It also encrypts the 
connection and is used during the Arab spring to reach social networks. This 
TOR project is also known of the `Dark net' and certain illegal activities.

For our purpose we choose to use the TOR network because we can programmaticaly
change the route after a certain requests and free VPN servers are slow and 
limited available.



\section{Integration}
People, articles and venues are retrieved
from multiple source. To be able to work with these entities, we need to 
integrate these sources in one model. To make this possible, we need to 
unqiquely identify the entities.

Again refering to Figure~\ref{fig:dataflow_jm2017}, we can consider this the 
tilde sign.

In the ideal situation al entities have an unique identifier which is 
independent of the source. However, in practice it is a bit more complex than 
that.



\subsection{Articles}
For all chapters from Springer LNCS we scraped the unique document identifier 
(DOI). DOI should be unique of all documents.

However, an analysis of the DBLP data set leaves us with a few DOI's which refer
to mutliple articles. The number of articles this occurs, is very low.

DBLP also has a lot of documents where the DOI is unknown; so data completeness 
is an issue here. \todo{Hoe veel dan? Iets van aantallen?}

For references we use opencitations. Because opencitations is DOI based, it is 
very good integreerbaar with LNCS.

% === oud, weg misschien
% However, this document id is not filled in for all titles. For
% the papers that do not have an doi, we can do the match on title. Unfortunately,
% this also is not that straightforward. The title sometimes contains latex 
% specific characters, or other weird characters.


\subsection{Uniquify people}
The author dataset contains all authors for every article. This means, if an 
author wrote multiple articles, it is multiple times present in this set. For
our use case, this is not sufficient; we need identify these people as the same 
person, otherwise collecting metrics for a person is impossible.

The set we achieved from LNCS conains name, orcid and email. An analysis of this
set made clear that we got the same name with multiple orcid, or one orcid that 
refers to more than one person. Concluding: a name does not make a person 
unique.

Luckely, DBLP acknowledges this problem. Although they do not yet have all 
authors unique, they are working on this problem and provide a way to get the 
unique persons already identified.

DBLP has the relation between author and article, so to get the `correct' author
of an article, we need to match the articles in DBLP with the chapters in LNCS.


% \subsection{Article integration}
% Not only do we need to match the articles of DBLP and Springer LNCS, we also 
% need to incorporate Aminer. Aminer is an enrichment of DBLP in that it adds
% citations. Although DBLP is working to incorporate citations in their dataset,
% this is still work in progress and Aminer has already a lot of that data.
% \todo{Stukje over Aminer}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=10cm]{images/linking_article_dblp_lncs_aminer.drawio.png}
%     \caption{Matching Springer LNCS, DBLP and Aminer}
%     \label{fig:linking_article_dblp_lncs_aminer}
% \end{figure}



\section{Nog onbenoemd}


To get an integrated dataset across all sources some challenges need to be 
addressed. Also, to be able to integrate the sources where we need to get the
data from, we sometimes need to get data from another source.

The main entities we need to integrate are person (authors, members of program
committees etc.) and articles (chapters from lncs with papers from aminers).

\section{Integration of persons}

First challenge is to make the authors unique. This comes with the
second challenge of data quality.

For example, from springer we scrape get the chapters (inproceedings) and their 
authors from their site. This authors sometimes comes with an orcid, which 
should be unique. However, we found situations where an author gets an orcid of
someone else. This is a data quality issue.
\todo{voorbeeld toevoegen}
Luckely, this does not occur much, and for the few it occurs we can easily 
create a manual file to override this. Of course with caution; if two orcid 
have the same name, we can not override this; some authors have the same name.
On the other hand, if one orcid refers to a totally different name, we can.


Seconds challenge is that a name does not make an author unique. The same name
refers to other authors. Luckely, DBLP addresses this problem by suffixing the
name with a four digit code~\footnote{\url{https://dblp.org/faq/1474704.html}}.
DBLP is making progress here, but this is not entirely done yet. But even if
they mention to unique identity authors, the match to authors in other 
datasources (like Aminer and Springer) is not straightforward. In that case we 
need to ignore the authors from these sources and make the match by going 
through the paper (or chapter, as is it is called within LNCS Springer).

Third challenge is the name. For most authors these sources do not contain an
orcid or an email address, which means we need to match the authors on name.
But, in some sources the names are fully written (with or without the 
middlename), sometimes the middlename only has a initial while the firstname is 
fully written, and sometimes we only get the initials and lastname.
DBLP covers this by naming all aliases of a single person.

Still we do have a problem, because inthe end we need to match these authors
with members we get from the frontmatters of lncs. Within this frontmatters, 
there is no orcid, email, or a four digit number to match to dblp records; these
documents or not written with the intention to identify persons.

\subsection{Approach}
\begin{itemize}
    \item Get DBLP data dump and create unique authors. DBLP also has their 
        aliases and sometimes orcid.
    \item Also use this dump to get the inproceedings, incollections and 
        articles.
    \item This dump can also be used to create the author relationship between 
        articles and persons.
    \item Integrate springer lncs chapters into the article entity.
    \item Use springer lncs to get the book (collection) and identify the 
        relationship between chapter and book.
    \item Integrate members from the frontmatter into the person entity.
    \item Use he frontmatter to lay the relationship between the proceeding and
        the person.
    \item *** Now we can get a few groups
    \item Integrate the aminer papers in the article entity.
    \item Use aminer to get the relationship between articles.
\end{itemize}


% ==============================================================================
\chapter{Data integration}
% ==============================================================================
\outline{
    \begin{itemize}
        \item Three main sets to intergrate: Humans, Venues, Publications
    \end{itemize}
    \section{Publication}
    \begin{itemize}
        \item Root is DBLP
        \item Key is DBLP key
        \item We need to integrate the publications from DBLP with Aminer
        \item DBLP is leading, so we need to match publications from dblp to aminer
        \item In two steps: First try to match on DOI, second, match on title
    \end{itemize}
    \subsection{Document Object Identifier}
    \begin{itemize}
        \item Wat is het? DOI is unique identifier of a document
        \item 
    \end{itemize} 
    \subsection{Enrichtment of DOI's}
    Some publication do not have an DOI in DBLP, but provide an additional link to arxiv site, or wikidata
    \paragraph{Arxiv}
    \begin{itemize}
        \item Arxiv sometimes has a DOI (but not always).
        \item We extact docuemnt information from the Arxiv API. As we were there, we also extracted the authors.
    \end{itemize}
    \paragraph{Wikidata}
}



\outline{
\begin{itemize}
    \item Onderzoeksvraag: Welke data is nodig en hoe kom je eraan?
    \item Doelstellingen:
    \begin{itemize}
        \item Data verzamelen
        \item Data analyseren
    \end{itemize}
\end{itemize}
}




% % ==============================================================================
% \chapter{Data Acquisition}
% % ==============================================================================
% \outline{


% \outline{


% }

% \section{Our root: Computing Research and Education Association of Australasia ranking}
% \begin{itemize}
%     \item CORE
%     \item Australian based, so might be biased
%     \item A website which ranks conferences and journals
%     \item Ranking is from A* to C. Our focus is for the ranking A*, A and B
%     \item The conference list contains the following attributes:
%     \begin{itemize}
%         \item Title
%         \item Acronym
%         \item Rank
%         \item DBLP link
%     \end{itemize}
% \end{itemize}
% \subsection{Gathering the data}
% \begin{itemize}
%     \item Choices for data acquisition:
%     \begin{itemize}
%         \item Use the export functionality on the website
%         \item Scrape the data from the HTML pages
%     \end{itemize}
%     \subsection{Methods}
%     \paragraph{Export functionality}
%     The core websites offers the functionality to download all the data in a CSV file. This is very useful, because 1) this allows us to get the data in a format that is simple to process in a database (we can consider this a table) and 2) it is fast to get the data.
    
%     The drawback however is that this data does not contain the DBLP url. This means we are not able to find related data in a structured manner.
%     A possible way to work with this problem is to manually add the dblp url's ourselves, but this is a labour intensive job, comes with a cost in reproducability and, because of the manual effort, is error-prone.
    
%     \paragraph{Webscraping}
%     \begin{itemize}
%         \item The core website contains the data in a HTML table accross multiple pages.
%         \item With GET request able to get this -> so we are able to automate this.
%         \item All data that is in the table is at our disposal.
%         \item Also the dblp url in the href of the Anchor tag.
%         \item Drawback: Need to build a script for it, which take more time than simply click the export button.
%     \end{itemize}
    
%     \paragraph{Copy-pasting from the website}
%     \begin{itemize}
%         \item Html table, so easy to paste in a table
%         \item Manual work: not convenient for reproducability
%         \item Also unable to find the DBLP url (we only get the 'view'-text in this case)
%     \end{itemize}
    
%     \paragraph{Method conclusion}
%     \begin{itemize}
%         \item We choose to build a script to scrape the data over downloading the export
%         \item Main reason was reproducability and less error prone in finding the DBLP urls.
%     \end{itemize}
    
%     \subsection{Process}
%     \begin{itemize}
%         \item pagination is given in the url
%         \item While testing the site we discovered that when requesting a page outside the pagination number, we did not get a 200 (OK) response
%         \item Loop through pages untill we do not get a 200 (OK) response
%         \item Get the data from the table
%         \item Write it to database
%         \item build in python because... not really a reason except it's hip and happening (and I honestly don't know why)
        
%     \end{itemize}
    
%     \subsection{Result}
%     \item The result was the data presented in two tables in schema 'core': 1) conf-ranks (conference rankings) and jnl-ranks (journal rankings)
%     \paragraph{Completeness}
%     Core conference data:
%     \begin{itemize}
%         \item 883 rows in total, within scope (because of rank): 486
%         \item No DBLP link for 24 conferences within our ranking focus.
%         \item To get this data we manually looked up the dblp url and added this data to the database. We were able to to find the dblp urls of 17 conferences, which remains 7 unknown
%     \end{itemize}
    
    
%     \item From this point we split the data acquisition process in two lanes:
%     \begin{itemize}
%         \item Conference tracks
%         \item Journal tracks
%     \end{itemize}
    
    
% \end{itemize}


% \section{Conference track}
% \begin{itemize}
%     \item Input is conf-ranks form core combined with manual searched DBLP urls
%     \item Purpose is to get the proceedings of the conferences
%     \item One conferences has one or more proceedings
%     \item DBLP contains this data
% \end{itemize}

% \subsection{Gathering DBLP data}
% \paragraph{Methods}
% \begin{itemize}
%     \item Download DBLP database
%     \begin{itemize}
%         \item Already done in project preparation
%         \item BUT; looking for certain proceedings were not available in download, but do on web
%     \end{itemize}
%     \item Use the API
%     \begin{itemize}
%         \item DBLP offers an API to download information.
%     \end{itemize}
% \end{itemize}
% We used the API to extract the data. Because the data is more complete.

% For all conferences in the core table (combined with some manual adjustment), we extracted the proceedings from DBLP.
% This resulted in 15994 proceedings.

% Looking for the most used publisers:
% \begin{table}[h]
% \begin{tabular}{ll}
% \hline
% Publisher & \# of proceedings \\ \hline
% Springer  & 4674              \\
% ACM       & 4010              \\
% IEEE *    & 3592             
% \end{tabular}
% \end{table}
% *IEEE also includes IEEE Computer Society
% } % END OUTLINE

% \section{PDF Reading}
% \outline{
% \begin{itemize}
%     \item Uitdiepen complexheid PDF extractie
% \end{itemize}
% }
% \begin{itemize}
%     \item PDF is mostly text plotted on a sheet with certain coordinates and properties. (also images, but we are not interested in these)
%     \item options: PDF to html
%     \begin{itemize}
%         \item Problem changed from interpreting PDF to interpreting HTML
%         \item Still need to be able to 'understand' the relationships between certain elements on the page
%     \end{itemize}
%     \item OCR (tesseract)
%     \begin{itemize}
%         \item OCR is a technology to detect characters from images
%         \item PDF is not an image, so first need to convert the PDF's to images
%         \item Output is plain text or TSV. Lot of contect (like position) gets lost in the process. This information is crucial if we want to interpret the pages.
%     \end{itemize}
% \end{itemize}
% \subsection{pdf to html}
% \begin{itemize}
%     \item Volgende libraries geprobeerd:
%     \begin{itemize}
%         \item PDFMiner
%         \begin{itemize}
%             \item Tries to split blocks of text
%             \item can be nice, but in these kind of documents, there is mostly a relation between the columne (like 'person' and an 'affiliation')
%         \end{itemize}
%         \item pdf2htmlex
%         \begin{itemize}
%             \item Most promising
%             \item Lots of CSS
%             \item Spaces can be tricky. Sometimes there is a space, but with a margin class with a negative number, so letter-space minus the margin, becomes so little, we have to ignore this space.
%             \item on the other hand, sometimes there is no space, but the space needs to be derived from the letter-spacing.
%             \item other challenge are items in a column that do not fit the row, so it continues on the next row. How do we consider this text as part of which column?
%         \end{itemize}
%     \end{itemize}
% \end{itemize}
% \subsection{Interpreting HTML}
% \begin{itemize}
%     \item what is the scope of the document we need? For LNCS, Springer provides a template where this section is calles 'Organization'. This is not used in all cases. From whcih point in time did they choose to use this template? Are there other keywords we can use? (like of a lot of subsection headers contain words like 'committee' or 'chair').
%     \item Can we set the scope by interpreting the format? Most of the lists with names are columns.
%     \item When do we consider an element an affiliation? it seems that most (if not all) affiliation have a comma to wplit the country name.
%     \item Can we consider all elements in a column if most of them have a comma as affiliation? No, sometimes names are also <lastname>, <firstname>. But when this is the case, this happens at the first column also, and affiliatons are not likely to be placed in the first column.
% \end{itemize}
% Approach 1: Directly raed HTML elements and see if we can make this work for the LNCS frontmatters.
% \begin{itemize}
%     \item Not sustainable
%     \item hard to maintain
%     \item lots of if-then rules
%     \item good first step to see what is working and what we need to take into account
% \end{itemize}
% Approach 2: First create a model of the HTML document, from this model create a graph with sectionheaders and sections.
% \begin{itemize}
%     \item How to detect section headers? Option; get the font-size that is most used. consider this as the size of the content. Everything that is bigger, assume headers.
% \end{itemize}


% ==============================================================================
\chapter{Conclusions}
\label{chp:conclusions}
% ==============================================================================
\outline{
\begin{itemize}
    \item Samenvatten
    \item Niet alleen samenvatting!
    \item Wat is de impact op de wereld? Wereldvrede :)
    \item Wat zouden we moeten doen en hoe kunnen hiervan profiteren?
    \item Misschien combineren met Discussions?
    \item Voorbeeld discussions
    \begin{itemize}
        \item Op generieke manier wetenschappelijk fraude detecteren, kan dit automatisch?
        \item Misschien meer data
        \item Imago schade als publishers niet meewerken
    \end{itemize}
    \item Misschien combineren met recommendations
    \begin{itemize}
        \item Informatie machine-readable aanleveren
    \end{itemize}
\end{itemize}
\section{Discussion}
One way to decrease fraud in the publication process is to step away from the
current way of judging a researcher. The University of Utrecht is moving to 
another method to judge their researchers based on their effort to promote open
research and their teamwork 
\footnote{\url{https://www.nature.com/articles/d41586-021-01759-5}}. With this 
movement the university is abandoning the indices based on publications and 
citations.
\begin{itemize}
    \item Hoe kan men dan oordelen waar budget heen moet?
    \item Hoe beoordeelt men de toegevoegde waarde van een onderzoeker 
    objectief?
    \item Wat is de impact hier van op de 'piramide'-structuur van 
    universiteiten?
    \item Hoe kunnen medewerkers bij de UU de overstap maken naar een 
    universiteit die beoordelingen wel basseren op impact factor?
    
\end{itemize}
}

% ==============================================================================
\chapter{Recommendations}
\label{chp:recommendations}
% ==============================================================================
\outline {
\paragraph{meer data openbaar maken}
\begin{itemize}
    \item bijvoorbeeld publicatielag
    \item Niet spannend (weinig moeite voor publishers), wel grote verrijking
\end{itemize}
\paragraph{Meer fraud detectie inzetten door publishers}
\begin{itemize}
    \item Kwaliteit van dit stukje van het publicatieprocess borgen.
\end{itemize}
}

% ==============================================================================
\chapter{Referenties}
% ==============================================================================

% ==============================================================================
\chapter{Appendices}
% ==============================================================================
\outline{
Saaie dingen die er wel in moeten
}

% ==============================================================================
% End of the line.... woohooo!
% ==============================================================================

\backmatter
\pagenumbering{roman}

\bibliographystyle{alpha}
\bibliography{report}

%% Use letters for the chapter numbers of the appendices.
%\appendix
\appendix
%  \input{Appendices/appendix-a}
%  \input{Appendices/appendix-b}
%  \input{Appendices/appendix-c}


\end{document}
